{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "from mxnet import ndarray as nd\n",
    "from mxnet.gluon import nn, rnn\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "from preprocessing_zh import Corpus, LMDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def try_gpu():\n",
    "    \"\"\"If GPU is available, return mx.gpu(0); else return mx.cpu()\"\"\"\n",
    "    try:\n",
    "        ctx = mx.gpu()\n",
    "        _ = nd.array([0], ctx=ctx)\n",
    "    except:\n",
    "        ctx = mx.cpu()\n",
    "    return ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context = try_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LMConfig(object):\n",
    "    rnn_type = 'LSTM'\n",
    "    embedding_dim = 200\n",
    "    hidden_dim = 200\n",
    "    num_layers = 2\n",
    "    dropout = 0.5\n",
    "    \n",
    "    batch_size = 20\n",
    "    seq_len = 30\n",
    "    learning_rate = 1.\n",
    "    optimizer = 'sgd'\n",
    "    grad_clip = 0.25\n",
    "    \n",
    "    tie_weights = True\n",
    "    \n",
    "    num_epochs = 2\n",
    "    print_per_batch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNModel(nn.Block):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        \n",
    "        vocab_size = config.vocab_size\n",
    "        embedding_dim = config.embedding_dim\n",
    "        hidden_dim = config.hidden_dim\n",
    "        dropout = config.dropout\n",
    "        num_layers = config.num_layers\n",
    "        rnn_type = config.rnn_type\n",
    "        tie_weights = config.tie_weights\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.drop = nn.Dropout(dropout)\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "            \n",
    "            if rnn_type in ['RNN', 'LSTM', 'GRU']:\n",
    "                self.rnn = getattr(rnn, rnn_type)(hidden_dim, num_layers, dropout=dropout)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid rnn_type %s. Options are RNN, LSTM, GRU\" % rnn_type)\n",
    "                \n",
    "            if tie_weights:\n",
    "                self.decoder = nn.Dense(vocab_size, params=self.encoder.params)\n",
    "            else:\n",
    "                self.decoder = nn.Dense(vocab_size)\n",
    "            \n",
    "    def forward(self, inputs, hidden):\n",
    "        embedded = self.drop(self.embedding(inputs))\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        decoded = self.decoder(output.reshape((-1, self.hidden_dim)))\n",
    "        return decoded, hidden\n",
    "    \n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.begin_state(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 242052, Vocabulary size: 3423.\n"
     ]
    }
   ],
   "source": [
    "train_dir = 'data/weicheng.txt'\n",
    "vocab_dir = 'data/weicheng.vocab.txt'\n",
    "\n",
    "corpus = Corpus(train_dir, vocab_dir)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of batches: 403, Batch Shape: (30, 20)\n"
     ]
    }
   ],
   "source": [
    "config = LMConfig()\n",
    "config.vocab_size = len(corpus.words)\n",
    "train_data = LMDataset(corpus.data, config.batch_size, config.seq_len)\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = RNNModel(config)\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(model.collect_params(), config.optimizer, {'learning_rate': config.learning_rate})\n",
    "loss_func = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detach(hidden):\n",
    "    if isinstance(hidden, (tuple, list)):\n",
    "        hidden = [i.detach() for i in hidden]\n",
    "    else:\n",
    "        hidden = hidden.detach()\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_time_dif(start_time):\n",
    "    \"\"\"\n",
    "    Return the time used since start_time.\n",
    "    \"\"\"\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(word_len=100):\n",
    "    start_index = np.random.randint(config.vocab_size)\n",
    "    word_list = [start_index]\n",
    "    \n",
    "    inputs = nd.array([word_list]).as_in_context(context)\n",
    "    hidden = model.begin_state(func=nd.zeros, batch_size=1, ctx=context)\n",
    "    \n",
    "    with autograd.record(train_mode=False):\n",
    "        for i in range(word_len):\n",
    "            hidden = detach(hidden)\n",
    "            output, hidden = model(inputs, hidden)\n",
    "            output_id = int(nd.argmax(output, 1).asscalar())\n",
    "            word_list.append(output_id)\n",
    "            inputs = nd.array([[output_id]]).as_in_context(context)\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 Batch 50] loss 7.34, perplexity 1534.54\n",
      "[Epoch 1 Batch 100] loss 5.86, perplexity 350.37\n",
      "[Epoch 1 Batch 150] loss 5.89, perplexity 362.17\n",
      "[Epoch 1 Batch 200] loss 5.92, perplexity 373.28\n",
      "[Epoch 1 Batch 250] loss 5.63, perplexity 278.89\n",
      "[Epoch 1 Batch 300] loss 5.34, perplexity 208.86\n",
      "[Epoch 1 Batch 350] loss 5.26, perplexity 192.52\n",
      "[Epoch 1 Batch 400] loss 5.37, perplexity 215.86\n",
      "酪，““我们们们们的““我们们们的““我们们们的““我们们们的““我们们们的““我们们们的““我们们们的““我们们们的““我们们们的““我们们们的““我们们们的““我们们们的““我们们们的““我们们们的\n",
      "[Epoch 2 Batch 50] loss 5.65, perplexity 284.80\n",
      "[Epoch 2 Batch 100] loss 4.88, perplexity 131.45\n",
      "[Epoch 2 Batch 150] loss 5.10, perplexity 163.92\n",
      "[Epoch 2 Batch 200] loss 5.23, perplexity 187.66\n",
      "[Epoch 2 Batch 250] loss 5.00, perplexity 148.11\n",
      "[Epoch 2 Batch 300] loss 4.76, perplexity 116.19\n",
      "[Epoch 2 Batch 350] loss 4.74, perplexity 114.19\n",
      "[Epoch 2 Batch 400] loss 4.98, perplexity 144.98\n",
      "苍，他们说：“我们说，我们说：“我们说，我们说：“我们说，我们说：“我们说，我们说：“我们说，我们说：“我们说，我们说：“我们说，我们说：“我们说，我们说：“我们说，我们说：“我们说，我们说：“我们说，\n",
      "[Epoch 3 Batch 50] loss 5.28, perplexity 195.92\n",
      "[Epoch 3 Batch 100] loss 4.56, perplexity 95.27\n",
      "[Epoch 3 Batch 150] loss 4.82, perplexity 123.73\n",
      "[Epoch 3 Batch 200] loss 4.99, perplexity 146.49\n",
      "[Epoch 3 Batch 250] loss 4.74, perplexity 114.02\n",
      "[Epoch 3 Batch 300] loss 4.53, perplexity 92.76\n",
      "[Epoch 3 Batch 350] loss 4.52, perplexity 92.24\n",
      "[Epoch 3 Batch 400] loss 4.78, perplexity 119.58\n",
      "散，他们常说：“我们说，我们说，我们说，我们说，我们说，我们说，我们说，我们说，我们说，我们说，我们说，我们说，我们说，我们说，我们说，我们说，我们说，我们说，我们说，我们说，我们说，我们说，我们说，我\n",
      "[Epoch 4 Batch 50] loss 5.10, perplexity 164.36\n",
      "[Epoch 4 Batch 100] loss 4.40, perplexity 81.82\n",
      "[Epoch 4 Batch 150] loss 4.65, perplexity 105.01\n",
      "[Epoch 4 Batch 200] loss 4.83, perplexity 125.74\n",
      "[Epoch 4 Batch 250] loss 4.58, perplexity 97.14\n",
      "[Epoch 4 Batch 300] loss 4.39, perplexity 80.33\n",
      "[Epoch 4 Batch 350] loss 4.40, perplexity 81.74\n",
      "[Epoch 4 Batch 400] loss 4.66, perplexity 105.59\n",
      "反正，他们说“痴气”，他们说：“我们不是他们的人，他们说：“我们不是他们的人，他们说：“我们不是他们的人，他们说：“我们不是他们的人，他们说：“我们不是他们的人，他们说：“我们不是他们的人，他们说：“我\n",
      "[Epoch 5 Batch 50] loss 4.99, perplexity 146.71\n",
      "[Epoch 5 Batch 100] loss 4.30, perplexity 73.34\n",
      "[Epoch 5 Batch 150] loss 4.55, perplexity 94.78\n",
      "[Epoch 5 Batch 200] loss 4.72, perplexity 112.66\n",
      "[Epoch 5 Batch 250] loss 4.47, perplexity 87.64\n",
      "[Epoch 5 Batch 300] loss 4.29, perplexity 73.16\n",
      "[Epoch 5 Batch 350] loss 4.32, perplexity 75.16\n",
      "[Epoch 5 Batch 400] loss 4.57, perplexity 96.62\n",
      "茂，他们俩常说：“我们在我们的信，我们说：“我们在我们的信，我们说：“我们在我们的信，我们说：“我们在我们的信，我们说：“我们在我们的信，我们说：“我们在我们的信，我们说：“我们在我们的信，我们说：“我\n",
      "[Epoch 6 Batch 50] loss 4.90, perplexity 134.23\n",
      "[Epoch 6 Batch 100] loss 4.22, perplexity 67.71\n",
      "[Epoch 6 Batch 150] loss 4.48, perplexity 87.98\n",
      "[Epoch 6 Batch 200] loss 4.66, perplexity 105.32\n",
      "[Epoch 6 Batch 250] loss 4.41, perplexity 82.02\n",
      "[Epoch 6 Batch 300] loss 4.23, perplexity 68.55\n",
      "[Epoch 6 Batch 350] loss 4.26, perplexity 70.89\n",
      "[Epoch 6 Batch 400] loss 4.50, perplexity 90.36\n",
      "菱，他们俩常说：“我们在牛津，我们在我们同学，就是我们的同学，就是个女儿，他们常说：“我们在牛津，我们在我们同学，就是我们的同学，就是个女儿，他们常说：“我们在牛津，我们在我们同学，就是我们的同学，就是\n",
      "[Epoch 7 Batch 50] loss 4.84, perplexity 125.95\n",
      "[Epoch 7 Batch 100] loss 4.17, perplexity 64.40\n",
      "[Epoch 7 Batch 150] loss 4.42, perplexity 82.83\n",
      "[Epoch 7 Batch 200] loss 4.58, perplexity 97.60\n",
      "[Epoch 7 Batch 250] loss 4.35, perplexity 77.70\n",
      "[Epoch 7 Batch 300] loss 4.18, perplexity 65.30\n",
      "[Epoch 7 Batch 350] loss 4.21, perplexity 67.36\n",
      "[Epoch 7 Batch 400] loss 4.45, perplexity 86.05\n",
      "溯我，我们就是个人，就是个女儿，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他\n",
      "[Epoch 8 Batch 50] loss 4.78, perplexity 118.76\n",
      "[Epoch 8 Batch 100] loss 4.12, perplexity 61.47\n",
      "[Epoch 8 Batch 150] loss 4.36, perplexity 78.51\n",
      "[Epoch 8 Batch 200] loss 4.54, perplexity 93.43\n",
      "[Epoch 8 Batch 250] loss 4.31, perplexity 74.67\n",
      "[Epoch 8 Batch 300] loss 4.14, perplexity 62.58\n",
      "[Epoch 8 Batch 350] loss 4.17, perplexity 64.91\n",
      "[Epoch 8 Batch 400] loss 4.41, perplexity 82.32\n",
      "属，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩\n",
      "[Epoch 9 Batch 50] loss 4.72, perplexity 111.73\n",
      "[Epoch 9 Batch 100] loss 4.08, perplexity 59.43\n",
      "[Epoch 9 Batch 150] loss 4.32, perplexity 74.95\n",
      "[Epoch 9 Batch 200] loss 4.50, perplexity 89.69\n",
      "[Epoch 9 Batch 250] loss 4.27, perplexity 71.41\n",
      "[Epoch 9 Batch 300] loss 4.10, perplexity 60.54\n",
      "[Epoch 9 Batch 350] loss 4.13, perplexity 62.20\n",
      "[Epoch 9 Batch 400] loss 4.38, perplexity 79.90\n",
      "燧，他们俩的父亲和我们俩的作者，就是个女儿，他们俩的“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，\n",
      "[Epoch 10 Batch 50] loss 4.69, perplexity 108.62\n",
      "[Epoch 10 Batch 100] loss 4.04, perplexity 57.07\n",
      "[Epoch 10 Batch 150] loss 4.30, perplexity 73.44\n",
      "[Epoch 10 Batch 200] loss 4.45, perplexity 85.98\n",
      "[Epoch 10 Batch 250] loss 4.23, perplexity 68.43\n",
      "[Epoch 10 Batch 300] loss 4.07, perplexity 58.49\n",
      "[Epoch 10 Batch 350] loss 4.11, perplexity 60.78\n",
      "[Epoch 10 Batch 400] loss 4.35, perplexity 77.58\n",
      "枝，他们俩说，我们在上海的时候，他们俩常常说，他们俩常说，我们在牛津时，他们俩常常说，我们在《围城》，他们俩常说，他们俩常说，我们在《围城》，他们俩常说，他们俩常说，我们在《围城》，他们俩常说，他们俩常\n",
      "[Epoch 11 Batch 50] loss 4.66, perplexity 105.32\n",
      "[Epoch 11 Batch 100] loss 4.02, perplexity 55.62\n",
      "[Epoch 11 Batch 150] loss 4.26, perplexity 71.08\n",
      "[Epoch 11 Batch 200] loss 4.42, perplexity 83.35\n",
      "[Epoch 11 Batch 250] loss 4.22, perplexity 67.91\n",
      "[Epoch 11 Batch 300] loss 4.04, perplexity 56.72\n",
      "[Epoch 11 Batch 350] loss 4.09, perplexity 59.45\n",
      "[Epoch 11 Batch 400] loss 4.32, perplexity 74.88\n",
      "础，就是个女儿，他们俩常常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们\n",
      "[Epoch 12 Batch 50] loss 4.63, perplexity 102.25\n",
      "[Epoch 12 Batch 100] loss 3.99, perplexity 54.06\n",
      "[Epoch 12 Batch 150] loss 4.24, perplexity 69.33\n",
      "[Epoch 12 Batch 200] loss 4.41, perplexity 82.11\n",
      "[Epoch 12 Batch 250] loss 4.19, perplexity 66.08\n",
      "[Epoch 12 Batch 300] loss 4.02, perplexity 55.65\n",
      "[Epoch 12 Batch 350] loss 4.06, perplexity 57.83\n",
      "[Epoch 12 Batch 400] loss 4.30, perplexity 74.07\n",
      "罪，他们俩常把我讲，他们俩常常说：“我们这次是个儿子，就是个女儿，就是个女儿，就是个女儿，他们俩常常说：“我们这次是个儿子，就是个女儿，就是个女儿，就是个女儿，他们俩常常说：“我们这次是个儿子，就是个女\n",
      "[Epoch 13 Batch 50] loss 4.61, perplexity 100.34\n",
      "[Epoch 13 Batch 100] loss 3.97, perplexity 52.96\n",
      "[Epoch 13 Batch 150] loss 4.21, perplexity 67.13\n",
      "[Epoch 13 Batch 200] loss 4.38, perplexity 80.21\n",
      "[Epoch 13 Batch 250] loss 4.17, perplexity 64.78\n",
      "[Epoch 13 Batch 300] loss 4.00, perplexity 54.43\n",
      "[Epoch 13 Batch 350] loss 4.03, perplexity 56.53\n",
      "[Epoch 13 Batch 400] loss 4.28, perplexity 72.18\n",
      "髓，不知道是个“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”\n",
      "[Epoch 14 Batch 50] loss 4.58, perplexity 97.17\n",
      "[Epoch 14 Batch 100] loss 3.95, perplexity 51.68\n",
      "[Epoch 14 Batch 150] loss 4.19, perplexity 66.19\n",
      "[Epoch 14 Batch 200] loss 4.36, perplexity 78.59\n",
      "[Epoch 14 Batch 250] loss 4.15, perplexity 63.28\n",
      "[Epoch 14 Batch 300] loss 3.98, perplexity 53.37\n",
      "[Epoch 14 Batch 350] loss 4.01, perplexity 55.32\n",
      "[Epoch 14 Batch 400] loss 4.26, perplexity 70.92\n",
      "评，他们俩就是个女儿，他们说：“我们这个孩子，就是个女儿，就是个女儿，就是个女儿，他们俩就是个女儿，他们说：“我们这个孩子，就是个女儿，就是个女儿，就是个女儿，他们俩就是个女儿，他们说：“我们这个孩子，\n",
      "[Epoch 15 Batch 50] loss 4.56, perplexity 95.49\n",
      "[Epoch 15 Batch 100] loss 3.92, perplexity 50.61\n",
      "[Epoch 15 Batch 150] loss 4.17, perplexity 64.64\n",
      "[Epoch 15 Batch 200] loss 4.34, perplexity 76.44\n",
      "[Epoch 15 Batch 250] loss 4.13, perplexity 62.12\n",
      "[Epoch 15 Batch 300] loss 3.96, perplexity 52.27\n",
      "[Epoch 15 Batch 350] loss 4.00, perplexity 54.57\n",
      "[Epoch 15 Batch 400] loss 4.24, perplexity 69.45\n",
      "耸了，他们俩的父亲也不敢再说，他们俩的父亲也不敢再说，他们俩的父亲也不敢再说，他们俩的父亲也不敢再说，他们俩的父亲也不敢再说，他们俩的父亲也不敢再说，他们俩的父亲也不敢再说，他们俩的父亲也不敢再说，他们\n",
      "[Epoch 16 Batch 50] loss 4.54, perplexity 94.04\n",
      "[Epoch 16 Batch 100] loss 3.91, perplexity 49.87\n",
      "[Epoch 16 Batch 150] loss 4.16, perplexity 63.79\n",
      "[Epoch 16 Batch 200] loss 4.33, perplexity 75.90\n",
      "[Epoch 16 Batch 250] loss 4.11, perplexity 60.76\n",
      "[Epoch 16 Batch 300] loss 3.94, perplexity 51.39\n",
      "[Epoch 16 Batch 350] loss 3.99, perplexity 54.15\n",
      "[Epoch 16 Batch 400] loss 4.22, perplexity 68.10\n",
      "拙，他们俩的父亲和钟书的“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“我们的父亲和钟书，他们俩家的“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“我们的父亲和钟书\n",
      "[Epoch 17 Batch 50] loss 4.53, perplexity 92.59\n",
      "[Epoch 17 Batch 100] loss 3.90, perplexity 49.63\n",
      "[Epoch 17 Batch 150] loss 4.15, perplexity 63.38\n",
      "[Epoch 17 Batch 200] loss 4.31, perplexity 74.47\n",
      "[Epoch 17 Batch 250] loss 4.10, perplexity 60.53\n",
      "[Epoch 17 Batch 300] loss 3.94, perplexity 51.42\n",
      "[Epoch 17 Batch 350] loss 3.97, perplexity 52.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17 Batch 400] loss 4.23, perplexity 68.43\n",
      "侦探，他们俩说，他们俩说，他们说，他们说，他们说，他们说，他们说，他们说，他们说，他们说，他们说，他们说，他们说，他们说，他们说，他们说，他们说，他们说，他们说，他们说，他们说，他们说，他们说，他们说，\n",
      "[Epoch 18 Batch 50] loss 4.50, perplexity 90.25\n",
      "[Epoch 18 Batch 100] loss 3.88, perplexity 48.60\n",
      "[Epoch 18 Batch 150] loss 4.12, perplexity 61.81\n",
      "[Epoch 18 Batch 200] loss 4.30, perplexity 73.54\n",
      "[Epoch 18 Batch 250] loss 4.10, perplexity 60.37\n",
      "[Epoch 18 Batch 300] loss 3.92, perplexity 50.21\n",
      "[Epoch 18 Batch 350] loss 3.96, perplexity 52.64\n",
      "[Epoch 18 Batch 400] loss 4.20, perplexity 66.44\n",
      "吧，我就是个孩子，就是个人，就是个人，他们俩不知道，就是个人，他们俩不知道，就是个人，他们俩不知道，就是个人，他们俩不知道，就是个人，他们俩不知道，就是个人，他们俩不知道，就是个人，他们俩不知道，就是个\n",
      "[Epoch 19 Batch 50] loss 4.49, perplexity 88.75\n",
      "[Epoch 19 Batch 100] loss 3.87, perplexity 47.81\n",
      "[Epoch 19 Batch 150] loss 4.12, perplexity 61.72\n",
      "[Epoch 19 Batch 200] loss 4.27, perplexity 71.82\n",
      "[Epoch 19 Batch 250] loss 4.08, perplexity 59.10\n",
      "[Epoch 19 Batch 300] loss 3.91, perplexity 49.81\n",
      "[Epoch 19 Batch 350] loss 3.95, perplexity 51.95\n",
      "[Epoch 19 Batch 400] loss 4.20, perplexity 66.60\n",
      "蓝，他们俩的人都是个人，他们的人，钟书的父亲和我们同班上，他们俩常常常常常说：“我们那位小姐，我就是个儿子，我就是个孩子，就是个女儿，就是个女儿，就是个女儿，就是个女儿，就是个女儿，就是个人，他们俩不知\n",
      "[Epoch 20 Batch 50] loss 4.48, perplexity 88.41\n",
      "[Epoch 20 Batch 100] loss 3.87, perplexity 47.84\n",
      "[Epoch 20 Batch 150] loss 4.11, perplexity 60.66\n",
      "[Epoch 20 Batch 200] loss 4.27, perplexity 71.35\n",
      "[Epoch 20 Batch 250] loss 4.06, perplexity 58.06\n",
      "[Epoch 20 Batch 300] loss 3.90, perplexity 49.52\n",
      "[Epoch 20 Batch 350] loss 3.95, perplexity 51.82\n",
      "[Epoch 20 Batch 400] loss 4.18, perplexity 65.45\n",
      "例如何，他们俩常常常常说，他们俩常常常常，他们俩常常常常常说：“我们是个大嫂妈，我们在上海，他们俩还是个女儿，还是个女儿，还是“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气\n",
      "[Epoch 21 Batch 50] loss 4.46, perplexity 86.77\n",
      "[Epoch 21 Batch 100] loss 3.85, perplexity 46.97\n",
      "[Epoch 21 Batch 150] loss 4.09, perplexity 59.73\n",
      "[Epoch 21 Batch 200] loss 4.24, perplexity 69.13\n",
      "[Epoch 21 Batch 250] loss 4.06, perplexity 57.88\n",
      "[Epoch 21 Batch 300] loss 3.89, perplexity 48.87\n",
      "[Epoch 21 Batch 350] loss 3.93, perplexity 50.73\n",
      "[Epoch 21 Batch 400] loss 4.17, perplexity 64.55\n",
      "乩，不知道他们俩的钱，钟书的父亲和我们同班，他们俩常常常常说，他们俩常常说，他们俩常常说，他们俩常常说，他们俩常常说，他们俩常常说，他们俩常常说，他们俩常常说，他们俩常常说，他们俩常常说，他们俩常常说，\n",
      "[Epoch 22 Batch 50] loss 4.46, perplexity 86.48\n",
      "[Epoch 22 Batch 100] loss 3.84, perplexity 46.75\n",
      "[Epoch 22 Batch 150] loss 4.08, perplexity 59.43\n",
      "[Epoch 22 Batch 200] loss 4.25, perplexity 69.92\n",
      "[Epoch 22 Batch 250] loss 4.05, perplexity 57.28\n",
      "[Epoch 22 Batch 300] loss 3.90, perplexity 49.21\n",
      "[Epoch 22 Batch 350] loss 3.94, perplexity 51.39\n",
      "[Epoch 22 Batch 400] loss 4.15, perplexity 63.64\n",
      "韶关系，他们俩常常常常常说：“我们那位小姐不是那时候，我就是个孩子，就是我们的姑母，他们的家庭，他们的钱都是他们的姑母，他父亲不知道，就是他们的“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”\n",
      "[Epoch 23 Batch 50] loss 4.44, perplexity 84.67\n",
      "[Epoch 23 Batch 100] loss 3.84, perplexity 46.30\n",
      "[Epoch 23 Batch 150] loss 4.06, perplexity 58.24\n",
      "[Epoch 23 Batch 200] loss 4.24, perplexity 69.75\n",
      "[Epoch 23 Batch 250] loss 4.04, perplexity 56.84\n",
      "[Epoch 23 Batch 300] loss 3.88, perplexity 48.19\n",
      "[Epoch 23 Batch 350] loss 3.91, perplexity 50.08\n",
      "[Epoch 23 Batch 400] loss 4.15, perplexity 63.53\n",
      "恭维，但是个钟书，他父亲常常常常，他父亲常常常常，他们俩常常常常写了。        钟书的“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，\n",
      "[Epoch 24 Batch 50] loss 4.44, perplexity 84.67\n",
      "[Epoch 24 Batch 100] loss 3.82, perplexity 45.51\n",
      "[Epoch 24 Batch 150] loss 4.06, perplexity 58.19\n",
      "[Epoch 24 Batch 200] loss 4.22, perplexity 67.79\n",
      "[Epoch 24 Batch 250] loss 4.03, perplexity 56.33\n",
      "[Epoch 24 Batch 300] loss 3.86, perplexity 47.64\n",
      "[Epoch 24 Batch 350] loss 3.91, perplexity 49.88\n",
      "[Epoch 24 Batch 400] loss 4.15, perplexity 63.19\n",
      "垂头，不知道他的话，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他们俩常说，他\n",
      "[Epoch 25 Batch 50] loss 4.42, perplexity 83.10\n",
      "[Epoch 25 Batch 100] loss 3.81, perplexity 45.16\n",
      "[Epoch 25 Batch 150] loss 4.05, perplexity 57.59\n",
      "[Epoch 25 Batch 200] loss 4.21, perplexity 67.56\n",
      "[Epoch 25 Batch 250] loss 4.03, perplexity 56.00\n",
      "[Epoch 25 Batch 300] loss 3.86, perplexity 47.55\n",
      "[Epoch 25 Batch 350] loss 3.90, perplexity 49.20\n",
      "[Epoch 25 Batch 400] loss 4.13, perplexity 62.30\n",
      "园，他们俩不知道，不是他的“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气\n",
      "[Epoch 26 Batch 50] loss 4.42, perplexity 83.35\n",
      "[Epoch 26 Batch 100] loss 3.80, perplexity 44.87\n",
      "[Epoch 26 Batch 150] loss 4.04, perplexity 56.76\n",
      "[Epoch 26 Batch 200] loss 4.21, perplexity 67.42\n",
      "[Epoch 26 Batch 250] loss 4.03, perplexity 56.00\n",
      "[Epoch 26 Batch 300] loss 3.85, perplexity 46.98\n",
      "[Epoch 26 Batch 350] loss 3.89, perplexity 48.96\n",
      "[Epoch 26 Batch 400] loss 4.13, perplexity 62.49\n",
      "想，他们俩不知道。”        钟书的“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，\n",
      "[Epoch 27 Batch 50] loss 4.41, perplexity 82.35\n",
      "[Epoch 27 Batch 100] loss 3.80, perplexity 44.60\n",
      "[Epoch 27 Batch 150] loss 4.04, perplexity 57.09\n",
      "[Epoch 27 Batch 200] loss 4.19, perplexity 66.32\n",
      "[Epoch 27 Batch 250] loss 4.01, perplexity 55.35\n",
      "[Epoch 27 Batch 300] loss 3.85, perplexity 47.06\n",
      "[Epoch 27 Batch 350] loss 3.88, perplexity 48.63\n",
      "[Epoch 27 Batch 400] loss 4.13, perplexity 61.93\n",
      "尿，一个红雾，不知道他们俩的“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴\n",
      "[Epoch 28 Batch 50] loss 4.40, perplexity 81.47\n",
      "[Epoch 28 Batch 100] loss 3.79, perplexity 44.06\n",
      "[Epoch 28 Batch 150] loss 4.04, perplexity 56.65\n",
      "[Epoch 28 Batch 200] loss 4.19, perplexity 66.21\n",
      "[Epoch 28 Batch 250] loss 4.00, perplexity 54.38\n",
      "[Epoch 28 Batch 300] loss 3.84, perplexity 46.64\n",
      "[Epoch 28 Batch 350] loss 3.87, perplexity 48.03\n",
      "[Epoch 28 Batch 400] loss 4.11, perplexity 60.97\n",
      "佃，钟书的父亲和我父亲的父亲和我讲书，他们的父亲和我讲书，他们的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”\n",
      "[Epoch 29 Batch 50] loss 4.39, perplexity 80.48\n",
      "[Epoch 29 Batch 100] loss 3.78, perplexity 44.01\n",
      "[Epoch 29 Batch 150] loss 4.03, perplexity 56.27\n",
      "[Epoch 29 Batch 200] loss 4.18, perplexity 65.57\n",
      "[Epoch 29 Batch 250] loss 3.99, perplexity 54.31\n",
      "[Epoch 29 Batch 300] loss 3.84, perplexity 46.51\n",
      "[Epoch 29 Batch 350] loss 3.87, perplexity 48.13\n",
      "[Epoch 29 Batch 400] loss 4.11, perplexity 60.97\n",
      "耗子，他们俩家里的人都是个大学毕业，他们俩常来了，我们俩常说，我们同班上课，我们同班上课，我们同班上课，我们同班上课，我们同班上课，我们同班上课，我们同班上楼，我们在上海，我们同班上课，我们同班上课，我\n",
      "[Epoch 30 Batch 50] loss 4.37, perplexity 79.44\n",
      "[Epoch 30 Batch 100] loss 3.77, perplexity 43.28\n",
      "[Epoch 30 Batch 150] loss 4.02, perplexity 55.89\n",
      "[Epoch 30 Batch 200] loss 4.18, perplexity 65.47\n",
      "[Epoch 30 Batch 250] loss 4.00, perplexity 54.66\n",
      "[Epoch 30 Batch 300] loss 3.84, perplexity 46.31\n",
      "[Epoch 30 Batch 350] loss 3.87, perplexity 47.91\n",
      "[Epoch 30 Batch 400] loss 4.11, perplexity 60.80\n",
      "脚，一个佣人都是个儿子，钟书的“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“\n",
      "[Epoch 31 Batch 50] loss 4.38, perplexity 79.84\n",
      "[Epoch 31 Batch 100] loss 3.76, perplexity 43.08\n",
      "[Epoch 31 Batch 150] loss 4.02, perplexity 55.88\n",
      "[Epoch 31 Batch 200] loss 4.18, perplexity 65.21\n",
      "[Epoch 31 Batch 250] loss 3.99, perplexity 54.11\n",
      "[Epoch 31 Batch 300] loss 3.82, perplexity 45.65\n",
      "[Epoch 31 Batch 350] loss 3.85, perplexity 46.92\n",
      "[Epoch 31 Batch 400] loss 4.10, perplexity 60.14\n",
      "响，他们俩家里的“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气\n",
      "[Epoch 32 Batch 50] loss 4.39, perplexity 80.39\n",
      "[Epoch 32 Batch 100] loss 3.75, perplexity 42.62\n",
      "[Epoch 32 Batch 150] loss 4.01, perplexity 55.38\n",
      "[Epoch 32 Batch 200] loss 4.17, perplexity 64.79\n",
      "[Epoch 32 Batch 250] loss 3.98, perplexity 53.38\n",
      "[Epoch 32 Batch 300] loss 3.82, perplexity 45.62\n",
      "[Epoch 32 Batch 350] loss 3.85, perplexity 47.04\n",
      "[Epoch 32 Batch 400] loss 4.08, perplexity 59.40\n",
      "饼，不知道这是个女儿，钟书的“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴\n",
      "[Epoch 33 Batch 50] loss 4.37, perplexity 79.21\n",
      "[Epoch 33 Batch 100] loss 3.75, perplexity 42.38\n",
      "[Epoch 33 Batch 150] loss 4.00, perplexity 54.65\n",
      "[Epoch 33 Batch 200] loss 4.16, perplexity 64.01\n",
      "[Epoch 33 Batch 250] loss 3.98, perplexity 53.38\n",
      "[Epoch 33 Batch 300] loss 3.82, perplexity 45.44\n",
      "[Epoch 33 Batch 350] loss 3.86, perplexity 47.45\n",
      "[Epoch 33 Batch 400] loss 4.08, perplexity 59.34\n",
      "况一个女儿，他们的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴\n",
      "[Epoch 34 Batch 50] loss 4.37, perplexity 78.71\n",
      "[Epoch 34 Batch 100] loss 3.74, perplexity 42.04\n",
      "[Epoch 34 Batch 150] loss 4.01, perplexity 55.29\n",
      "[Epoch 34 Batch 200] loss 4.15, perplexity 63.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 34 Batch 250] loss 3.97, perplexity 52.83\n",
      "[Epoch 34 Batch 300] loss 3.81, perplexity 45.20\n",
      "[Epoch 34 Batch 350] loss 3.85, perplexity 46.94\n",
      "[Epoch 34 Batch 400] loss 4.07, perplexity 58.53\n",
      "女儿，钟书的父亲和叔父和钟书的“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“\n",
      "[Epoch 35 Batch 50] loss 4.35, perplexity 77.76\n",
      "[Epoch 35 Batch 100] loss 3.74, perplexity 42.10\n",
      "[Epoch 35 Batch 150] loss 4.00, perplexity 54.63\n",
      "[Epoch 35 Batch 200] loss 4.14, perplexity 63.10\n",
      "[Epoch 35 Batch 250] loss 3.95, perplexity 52.18\n",
      "[Epoch 35 Batch 300] loss 3.80, perplexity 44.89\n",
      "[Epoch 35 Batch 350] loss 3.84, perplexity 46.43\n",
      "[Epoch 35 Batch 400] loss 4.09, perplexity 59.45\n",
      "讽刺，不知道何同。他们俩俩俩家里的人都是个复合，他们俩不是“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴\n",
      "[Epoch 36 Batch 50] loss 4.36, perplexity 77.97\n",
      "[Epoch 36 Batch 100] loss 3.73, perplexity 41.79\n",
      "[Epoch 36 Batch 150] loss 4.00, perplexity 54.82\n",
      "[Epoch 36 Batch 200] loss 4.14, perplexity 62.78\n",
      "[Epoch 36 Batch 250] loss 3.96, perplexity 52.45\n",
      "[Epoch 36 Batch 300] loss 3.78, perplexity 43.98\n",
      "[Epoch 36 Batch 350] loss 3.84, perplexity 46.39\n",
      "[Epoch 36 Batch 400] loss 4.07, perplexity 58.32\n",
      "人，钟书的“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“\n",
      "[Epoch 37 Batch 50] loss 4.35, perplexity 77.14\n",
      "[Epoch 37 Batch 100] loss 3.73, perplexity 41.77\n",
      "[Epoch 37 Batch 150] loss 4.00, perplexity 54.34\n",
      "[Epoch 37 Batch 200] loss 4.12, perplexity 61.85\n",
      "[Epoch 37 Batch 250] loss 3.96, perplexity 52.58\n",
      "[Epoch 37 Batch 300] loss 3.79, perplexity 44.31\n",
      "[Epoch 37 Batch 350] loss 3.82, perplexity 45.65\n",
      "[Epoch 37 Batch 400] loss 4.06, perplexity 58.17\n",
      "差不知道，不是他的“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴\n",
      "[Epoch 38 Batch 50] loss 4.35, perplexity 77.83\n",
      "[Epoch 38 Batch 100] loss 3.73, perplexity 41.79\n",
      "[Epoch 38 Batch 150] loss 3.99, perplexity 53.87\n",
      "[Epoch 38 Batch 200] loss 4.13, perplexity 62.02\n",
      "[Epoch 38 Batch 250] loss 3.95, perplexity 51.82\n",
      "[Epoch 38 Batch 300] loss 3.79, perplexity 44.36\n",
      "[Epoch 38 Batch 350] loss 3.82, perplexity 45.72\n",
      "[Epoch 38 Batch 400] loss 4.06, perplexity 57.81\n",
      "圆，不知道何必是“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气\n",
      "[Epoch 39 Batch 50] loss 4.35, perplexity 77.40\n",
      "[Epoch 39 Batch 100] loss 3.72, perplexity 41.30\n",
      "[Epoch 39 Batch 150] loss 3.98, perplexity 53.50\n",
      "[Epoch 39 Batch 200] loss 4.14, perplexity 62.69\n",
      "[Epoch 39 Batch 250] loss 3.93, perplexity 51.04\n",
      "[Epoch 39 Batch 300] loss 3.78, perplexity 43.90\n",
      "[Epoch 39 Batch 350] loss 3.81, perplexity 45.08\n",
      "[Epoch 39 Batch 400] loss 4.05, perplexity 57.16\n",
      "诵，他们俩家里的人也不敢讲，他们的父亲和我父亲的父亲，还是“痴气”的“痴气”的“痴气”的“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴\n",
      "[Epoch 40 Batch 50] loss 4.34, perplexity 76.74\n",
      "[Epoch 40 Batch 100] loss 3.72, perplexity 41.20\n",
      "[Epoch 40 Batch 150] loss 3.97, perplexity 53.16\n",
      "[Epoch 40 Batch 200] loss 4.13, perplexity 62.00\n",
      "[Epoch 40 Batch 250] loss 3.94, perplexity 51.38\n",
      "[Epoch 40 Batch 300] loss 3.78, perplexity 43.70\n",
      "[Epoch 40 Batch 350] loss 3.81, perplexity 45.09\n",
      "[Epoch 40 Batch 400] loss 4.05, perplexity 57.68\n",
      "炭，不过，不是他的好朋友，他们的美丽表情，还是个复合，他父亲母亲母亲的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“\n",
      "[Epoch 41 Batch 50] loss 4.33, perplexity 76.01\n",
      "[Epoch 41 Batch 100] loss 3.71, perplexity 40.76\n",
      "[Epoch 41 Batch 150] loss 3.96, perplexity 52.47\n",
      "[Epoch 41 Batch 200] loss 4.13, perplexity 62.00\n",
      "[Epoch 41 Batch 250] loss 3.93, perplexity 50.76\n",
      "[Epoch 41 Batch 300] loss 3.78, perplexity 43.63\n",
      "[Epoch 41 Batch 350] loss 3.80, perplexity 44.89\n",
      "[Epoch 41 Batch 400] loss 4.06, perplexity 57.97\n",
      "晔，他父亲负担不住，他父亲负担不住，他父亲负担不住，他父亲负担不住，他父亲负担不住，他父亲负担不住，他父亲负担不住，他父亲负担不住，他父亲负担不住，他父亲负担不住，他父亲负担不住，他父亲负担不住，他父亲\n",
      "[Epoch 42 Batch 50] loss 4.33, perplexity 75.97\n",
      "[Epoch 42 Batch 100] loss 3.72, perplexity 41.08\n",
      "[Epoch 42 Batch 150] loss 3.95, perplexity 52.16\n",
      "[Epoch 42 Batch 200] loss 4.12, perplexity 61.70\n",
      "[Epoch 42 Batch 250] loss 3.93, perplexity 50.90\n",
      "[Epoch 42 Batch 300] loss 3.77, perplexity 43.42\n",
      "[Epoch 42 Batch 350] loss 3.81, perplexity 44.95\n",
      "[Epoch 42 Batch 400] loss 4.05, perplexity 57.28\n",
      "鲜，他们俩的家境，一九三七年秋，他父亲常常常常常常常常常常常常常常常常常常常常常常常常常常常常常常常常常，他们俩不会再想。我们俩不是他的‘痴气’，就是个钟书，不知多少，只能当然不敢再想到了。      \n",
      "[Epoch 43 Batch 50] loss 4.33, perplexity 76.29\n",
      "[Epoch 43 Batch 100] loss 3.71, perplexity 40.70\n",
      "[Epoch 43 Batch 150] loss 3.96, perplexity 52.68\n",
      "[Epoch 43 Batch 200] loss 4.12, perplexity 61.42\n",
      "[Epoch 43 Batch 250] loss 3.93, perplexity 50.90\n",
      "[Epoch 43 Batch 300] loss 3.78, perplexity 43.70\n",
      "[Epoch 43 Batch 350] loss 3.81, perplexity 44.94\n",
      "[Epoch 43 Batch 400] loss 4.04, perplexity 56.95\n",
      "雷，他们俩不但他的意思，就是个复合的。他父亲常常常常，他们俩家里的人也不敢讲，他们俩家里的人也不敢再，钟书也许是“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，\n",
      "[Epoch 44 Batch 50] loss 4.33, perplexity 75.97\n",
      "[Epoch 44 Batch 100] loss 3.72, perplexity 41.17\n",
      "[Epoch 44 Batch 150] loss 3.97, perplexity 52.88\n",
      "[Epoch 44 Batch 200] loss 4.10, perplexity 60.46\n",
      "[Epoch 44 Batch 250] loss 3.94, perplexity 51.29\n",
      "[Epoch 44 Batch 300] loss 3.77, perplexity 43.24\n",
      "[Epoch 44 Batch 350] loss 3.80, perplexity 44.92\n",
      "[Epoch 44 Batch 400] loss 4.04, perplexity 56.74\n",
      "侍者，他父亲常说：“我们那时候不知道，还是个复合的。”        钟书的父亲和叔父和《围城》、《围城》、《围城》、《围城》、《围城》、《围城》、《围城》、《围城》、《围城》、《围城》、《围城》、《围\n",
      "[Epoch 45 Batch 50] loss 4.32, perplexity 75.49\n",
      "[Epoch 45 Batch 100] loss 3.72, perplexity 41.13\n",
      "[Epoch 45 Batch 150] loss 3.96, perplexity 52.39\n",
      "[Epoch 45 Batch 200] loss 4.10, perplexity 60.41\n",
      "[Epoch 45 Batch 250] loss 3.92, perplexity 50.45\n",
      "[Epoch 45 Batch 300] loss 3.75, perplexity 42.60\n",
      "[Epoch 45 Batch 350] loss 3.80, perplexity 44.55\n",
      "[Epoch 45 Batch 400] loss 4.04, perplexity 56.74\n",
      "石子，一个大学毕业，他父亲常说：“我们是个Cowrt!不是很好的，我就不是个大学毕业，他们的一篇文章，他们同学院长，他们的父亲和钟书的父亲和叔父已经衰盛，他父亲常常说：“我们是个Cowrt!不是很好的，\n",
      "[Epoch 46 Batch 50] loss 4.32, perplexity 74.86\n",
      "[Epoch 46 Batch 100] loss 3.71, perplexity 40.91\n",
      "[Epoch 46 Batch 150] loss 3.96, perplexity 52.22\n",
      "[Epoch 46 Batch 200] loss 4.10, perplexity 60.26\n",
      "[Epoch 46 Batch 250] loss 3.92, perplexity 50.32\n",
      "[Epoch 46 Batch 300] loss 3.76, perplexity 43.00\n",
      "[Epoch 46 Batch 350] loss 3.79, perplexity 44.42\n",
      "[Epoch 46 Batch 400] loss 4.03, perplexity 56.09\n",
      "末，他们俩家里的人也不是个大学毕业，他父亲负担，其实是“痴气”的“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”，“痴气”\n",
      "[Epoch 47 Batch 50] loss 4.30, perplexity 73.98\n",
      "[Epoch 47 Batch 100] loss 3.71, perplexity 41.06\n",
      "[Epoch 47 Batch 150] loss 3.95, perplexity 51.97\n",
      "[Epoch 47 Batch 200] loss 4.09, perplexity 59.97\n",
      "[Epoch 47 Batch 250] loss 3.91, perplexity 50.00\n",
      "[Epoch 47 Batch 300] loss 3.76, perplexity 43.03\n",
      "[Epoch 47 Batch 350] loss 3.79, perplexity 44.29\n",
      "[Epoch 47 Batch 400] loss 4.03, perplexity 56.10\n",
      "男儿，他们的家境不是他的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”\n",
      "[Epoch 48 Batch 50] loss 4.31, perplexity 74.31\n",
      "[Epoch 48 Batch 100] loss 3.70, perplexity 40.52\n",
      "[Epoch 48 Batch 150] loss 3.95, perplexity 52.00\n",
      "[Epoch 48 Batch 200] loss 4.09, perplexity 59.88\n",
      "[Epoch 48 Batch 250] loss 3.92, perplexity 50.51\n",
      "[Epoch 48 Batch 300] loss 3.75, perplexity 42.60\n",
      "[Epoch 48 Batch 350] loss 3.79, perplexity 44.13\n",
      "[Epoch 48 Batch 400] loss 4.03, perplexity 56.15\n",
      "罐，他们的家里也没有的。”        钟书的父亲和“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”\n",
      "[Epoch 49 Batch 50] loss 4.30, perplexity 73.66\n",
      "[Epoch 49 Batch 100] loss 3.71, perplexity 40.67\n",
      "[Epoch 49 Batch 150] loss 3.95, perplexity 51.71\n",
      "[Epoch 49 Batch 200] loss 4.08, perplexity 59.34\n",
      "[Epoch 49 Batch 250] loss 3.92, perplexity 50.23\n",
      "[Epoch 49 Batch 300] loss 3.75, perplexity 42.64\n",
      "[Epoch 49 Batch 350] loss 3.79, perplexity 44.11\n",
      "[Epoch 49 Batch 400] loss 4.02, perplexity 55.96\n",
      "陛得，不知道他们的家境。        钟书的父亲和叔父同乡，他父亲负担，他父亲负担年，他们俩不知道，不是写信，他们俩不知道，不是他的“痴气”，“痴气”的“痴气”的“痴气”的“痴气”的“痴气”的“痴气”\n",
      "[Epoch 50 Batch 50] loss 4.30, perplexity 73.39\n",
      "[Epoch 50 Batch 100] loss 3.71, perplexity 40.67\n",
      "[Epoch 50 Batch 150] loss 3.94, perplexity 51.20\n",
      "[Epoch 50 Batch 200] loss 4.07, perplexity 58.61\n",
      "[Epoch 50 Batch 250] loss 3.91, perplexity 49.72\n",
      "[Epoch 50 Batch 300] loss 3.75, perplexity 42.64\n",
      "[Epoch 50 Batch 350] loss 3.78, perplexity 43.97\n",
      "[Epoch 50 Batch 400] loss 4.02, perplexity 55.77\n",
      "箧，他们俩家里的人都是个博士，所以我们的父亲和叔叔说，他们的父亲和钟书的父亲和《七八岁），他父亲和钟书的父亲和叔父同学，他父亲和钟书的父亲和叔父同学，他父亲和钟书的父亲和“痴气”的“痴气”的“痴气”的“\n"
     ]
    }
   ],
   "source": [
    "grad_clip = config.grad_clip\n",
    "seq_len = config.seq_len\n",
    "batch_size = config.batch_size\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(50):\n",
    "    total_loss = 0.0\n",
    "    hidden = model.begin_state(func=nd.zeros, batch_size=batch_size, ctx=context)\n",
    "    for ibatch, (data, label) in enumerate(train_data):\n",
    "        data = nd.array(data).as_in_context(context)\n",
    "        label = nd.array(label).as_in_context(context)\n",
    "        hidden = detach(hidden)\n",
    "        \n",
    "        with autograd.record(train_mode=True):\n",
    "            output, hidden = model(data, hidden)\n",
    "            loss = loss_func(output, label)\n",
    "            \n",
    "        loss.backward()\n",
    "        \n",
    "        grads = [x.grad(context) for x in model.collect_params().values()]\n",
    "        gluon.utils.clip_global_norm(grads, grad_clip * seq_len * batch_size)\n",
    "        \n",
    "        trainer.step(config.batch_size)\n",
    "        total_loss += nd.sum(loss).asscalar()\n",
    "        \n",
    "        if ibatch % config.print_per_batch == 0 and ibatch > 0:\n",
    "            cur_loss = total_loss / seq_len / batch_size / config.print_per_batch\n",
    "            print('[Epoch %d Batch %d] loss %.2f, perplexity %.2f' % (\n",
    "                    epoch + 1, ibatch, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0.0\n",
    "    print(''.join(corpus.to_word(generate())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
