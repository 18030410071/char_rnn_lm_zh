{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dzkan\\Anaconda3\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py:46: DeprecationWarning: OpenSSL.rand is deprecated - you should use os.urandom instead\n",
      "  import OpenSSL.SSL\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "from mxnet import ndarray as nd\n",
    "from mxnet.gluon import nn, rnn\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "from preprocessing_zh import Corpus, LMDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def try_gpu():\n",
    "    \"\"\"If GPU is available, return mx.gpu(0); else return mx.cpu()\"\"\"\n",
    "    try:\n",
    "        ctx = mx.gpu()\n",
    "        _ = nd.array([0], ctx=ctx)\n",
    "    except:\n",
    "        ctx = mx.cpu()\n",
    "    return ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context = try_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LMConfig(object):\n",
    "    rnn_type = 'LSTM'\n",
    "    embedding_dim = 200\n",
    "    hidden_dim = 200\n",
    "    num_layers = 2\n",
    "    dropout = 0.5\n",
    "    \n",
    "    batch_size = 20\n",
    "    seq_len = 30\n",
    "    learning_rate = 1.\n",
    "    optimizer = 'sgd'\n",
    "    grad_clip = 0.25\n",
    "    \n",
    "    tie_weights = True\n",
    "    \n",
    "    num_epochs = 2\n",
    "    print_per_batch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNModel(nn.Block):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        \n",
    "        vocab_size = config.vocab_size\n",
    "        embedding_dim = config.embedding_dim\n",
    "        hidden_dim = config.hidden_dim\n",
    "        dropout = config.dropout\n",
    "        num_layers = config.num_layers\n",
    "        rnn_type = config.rnn_type\n",
    "        tie_weights = config.tie_weights\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.drop = nn.Dropout(dropout)\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "            \n",
    "            if rnn_type in ['RNN', 'LSTM', 'GRU']:\n",
    "                self.rnn = getattr(rnn, rnn_type)(hidden_dim, num_layers, dropout=dropout)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid rnn_type %s. Options are RNN, LSTM, GRU\" % rnn_type)\n",
    "                \n",
    "            if tie_weights:\n",
    "                self.decoder = nn.Dense(vocab_size, params=self.embedding.params, in_units=hidden_dim)\n",
    "            else:\n",
    "                self.decoder = nn.Dense(vocab_size)\n",
    "            \n",
    "    def forward(self, inputs, hidden):\n",
    "        embedded = self.drop(self.embedding(inputs))\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        decoded = self.decoder(output.reshape((-1, self.hidden_dim)))\n",
    "        return decoded, hidden\n",
    "    \n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.begin_state(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 608880, Vocabulary size: 4002.\n"
     ]
    }
   ],
   "source": [
    "train_dir = 'data/sanguoyanyi.txt'\n",
    "vocab_dir = 'data/sanguo.vocab.txt'\n",
    "\n",
    "corpus = Corpus(train_dir, vocab_dir)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of batches: 1014, Batch Shape: (30, 20)\n"
     ]
    }
   ],
   "source": [
    "config = LMConfig()\n",
    "config.vocab_size = len(corpus.words)\n",
    "train_data = LMDataset(corpus.data, config.batch_size, config.seq_len)\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = RNNModel(config)\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(model.collect_params(), config.optimizer, {'learning_rate': config.learning_rate})\n",
    "loss_func = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detach(hidden):\n",
    "    if isinstance(hidden, (tuple, list)):\n",
    "        hidden = [i.detach() for i in hidden]\n",
    "    else:\n",
    "        hidden = hidden.detach()\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_time_dif(start_time):\n",
    "    \"\"\"\n",
    "    Return the time used since start_time.\n",
    "    \"\"\"\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(word_len=100):\n",
    "    start_index = np.random.randint(config.vocab_size)\n",
    "    word_list = [start_index]\n",
    "    \n",
    "    inputs = mx.nd.array([word_list]).as_in_context(context)\n",
    "    hidden = model.begin_state(func=mx.nd.zeros, batch_size=1, ctx=context)\n",
    "    \n",
    "    with autograd.record(train_mode=False):\n",
    "        for i in range(word_len):\n",
    "            hidden = detach(hidden)\n",
    "            output, hidden = model(inputs, hidden)\n",
    "            # output_id = int(mx.nd.argmax(output, 1).asscalar())\n",
    "            output_id = mx.nd.random.multinomial(output[0].softmax()).asscalar()\n",
    "            word_list.append(output_id)\n",
    "            inputs = mx.nd.array([[output_id]]).as_in_context(context)\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 Batch 50] loss 7.37, perplexity 1591.70\n",
      "[Epoch 1 Batch 100] loss 6.44, perplexity 624.50\n",
      "[Epoch 1 Batch 150] loss 6.03, perplexity 416.08\n",
      "[Epoch 1 Batch 200] loss 5.84, perplexity 343.59\n",
      "[Epoch 1 Batch 250] loss 5.43, perplexity 227.66\n",
      "[Epoch 1 Batch 300] loss 5.55, perplexity 256.55\n",
      "[Epoch 1 Batch 350] loss 5.44, perplexity 231.60\n",
      "[Epoch 1 Batch 400] loss 5.27, perplexity 194.29\n",
      "[Epoch 1 Batch 450] loss 5.22, perplexity 185.21\n",
      "[Epoch 1 Batch 500] loss 5.18, perplexity 177.84\n",
      "[Epoch 1 Batch 550] loss 5.19, perplexity 179.10\n",
      "[Epoch 1 Batch 600] loss 5.19, perplexity 179.74\n",
      "[Epoch 1 Batch 650] loss 4.99, perplexity 146.25\n",
      "[Epoch 1 Batch 700] loss 5.23, perplexity 186.85\n",
      "[Epoch 1 Batch 750] loss 4.98, perplexity 145.70\n",
      "[Epoch 1 Batch 800] loss 5.07, perplexity 159.78\n",
      "[Epoch 1 Batch 850] loss 4.69, perplexity 108.45\n",
      "[Epoch 1 Batch 900] loss 4.87, perplexity 130.18\n",
      "[Epoch 1 Batch 950] loss 5.02, perplexity 150.75\n",
      "[Epoch 1 Batch 1000] loss 4.89, perplexity 132.36\n",
      "孱书巾逼。哺暗倒曰：“柯春弟诩子王公若何？”宴谏曰：“天下者。状惭喉中，遥决于地中。且侄下，便死降了，以吞木边骠阳。　　各路星船十万，方娶营口合取内文。贾充作宴，即遁圈期否！”皓大喜，遂飞然处势事；扬\n",
      "[Epoch 2 Batch 50] loss 5.39, perplexity 218.48\n",
      "[Epoch 2 Batch 100] loss 5.04, perplexity 154.61\n",
      "[Epoch 2 Batch 150] loss 4.91, perplexity 135.69\n",
      "[Epoch 2 Batch 200] loss 4.99, perplexity 147.64\n",
      "[Epoch 2 Batch 250] loss 4.59, perplexity 98.47\n",
      "[Epoch 2 Batch 300] loss 4.85, perplexity 128.17\n",
      "[Epoch 2 Batch 350] loss 4.82, perplexity 123.82\n",
      "[Epoch 2 Batch 400] loss 4.75, perplexity 115.49\n",
      "[Epoch 2 Batch 450] loss 4.67, perplexity 106.72\n",
      "[Epoch 2 Batch 500] loss 4.73, perplexity 113.06\n",
      "[Epoch 2 Batch 550] loss 4.79, perplexity 119.75\n",
      "[Epoch 2 Batch 600] loss 4.82, perplexity 123.67\n",
      "[Epoch 2 Batch 650] loss 4.64, perplexity 103.40\n",
      "[Epoch 2 Batch 700] loss 4.92, perplexity 136.86\n",
      "[Epoch 2 Batch 750] loss 4.66, perplexity 105.85\n",
      "[Epoch 2 Batch 800] loss 4.77, perplexity 118.13\n",
      "[Epoch 2 Batch 850] loss 4.40, perplexity 81.63\n",
      "[Epoch 2 Batch 900] loss 4.64, perplexity 103.10\n",
      "[Epoch 2 Batch 950] loss 4.77, perplexity 117.84\n",
      "[Epoch 2 Batch 1000] loss 4.67, perplexity 107.01\n",
      "审”持千兵到吴，前官又然曰：“南林晋兵不至也。”会大惊曰：“既折！”充默然宫中，拜封太傅也。今彼诸处人淮其地。晋主孙亮出朝责已毕。诞欲欲自换。　　谗之境也，卿助张悌于都督合应二人，大将军傅江山。后人有诗\n",
      "[Epoch 3 Batch 50] loss 5.19, perplexity 179.79\n",
      "[Epoch 3 Batch 100] loss 4.83, perplexity 124.73\n",
      "[Epoch 3 Batch 150] loss 4.74, perplexity 114.84\n",
      "[Epoch 3 Batch 200] loss 4.86, perplexity 129.47\n",
      "[Epoch 3 Batch 250] loss 4.45, perplexity 85.31\n",
      "[Epoch 3 Batch 300] loss 4.71, perplexity 111.07\n",
      "[Epoch 3 Batch 350] loss 4.69, perplexity 109.11\n",
      "[Epoch 3 Batch 400] loss 4.65, perplexity 104.81\n",
      "[Epoch 3 Batch 450] loss 4.55, perplexity 94.36\n",
      "[Epoch 3 Batch 500] loss 4.63, perplexity 102.07\n",
      "[Epoch 3 Batch 550] loss 4.68, perplexity 107.32\n",
      "[Epoch 3 Batch 600] loss 4.72, perplexity 112.67\n",
      "[Epoch 3 Batch 650] loss 4.55, perplexity 94.53\n",
      "[Epoch 3 Batch 700] loss 4.82, perplexity 124.16\n",
      "[Epoch 3 Batch 750] loss 4.57, perplexity 96.65\n",
      "[Epoch 3 Batch 800] loss 4.68, perplexity 108.06\n",
      "[Epoch 3 Batch 850] loss 4.34, perplexity 76.63\n",
      "[Epoch 3 Batch 900] loss 4.58, perplexity 97.06\n",
      "[Epoch 3 Batch 950] loss 4.68, perplexity 107.92\n",
      "[Epoch 3 Batch 1000] loss 4.61, perplexity 100.30\n",
      "乘）木箭，皆进兵邕，望去潘忠。兵到，不能弃上程崇金。其计势勿忧矣。今切酌累旧朝将文司马炎，遂孙皓顿手鸣巾火出，附顺安无水隐栏上一措而陷行。吴主令诸将出书，押其手食武将军士，知见公弥红灌，都督赶住人请后人\n",
      "[Epoch 4 Batch 50] loss 5.11, perplexity 166.34\n",
      "[Epoch 4 Batch 100] loss 4.76, perplexity 116.43\n",
      "[Epoch 4 Batch 150] loss 4.67, perplexity 106.80\n",
      "[Epoch 4 Batch 200] loss 4.80, perplexity 121.71\n",
      "[Epoch 4 Batch 250] loss 4.40, perplexity 81.36\n",
      "[Epoch 4 Batch 300] loss 4.66, perplexity 105.82\n",
      "[Epoch 4 Batch 350] loss 4.65, perplexity 104.21\n",
      "[Epoch 4 Batch 400] loss 4.60, perplexity 99.85\n",
      "[Epoch 4 Batch 450] loss 4.49, perplexity 89.07\n",
      "[Epoch 4 Batch 500] loss 4.60, perplexity 99.05\n",
      "[Epoch 4 Batch 550] loss 4.64, perplexity 103.35\n",
      "[Epoch 4 Batch 600] loss 4.69, perplexity 108.39\n",
      "[Epoch 4 Batch 650] loss 4.51, perplexity 90.73\n",
      "[Epoch 4 Batch 700] loss 4.79, perplexity 119.94\n",
      "[Epoch 4 Batch 750] loss 4.53, perplexity 92.46\n",
      "[Epoch 4 Batch 800] loss 4.65, perplexity 104.52\n",
      "[Epoch 4 Batch 850] loss 4.29, perplexity 73.22\n",
      "[Epoch 4 Batch 900] loss 4.53, perplexity 93.07\n",
      "[Epoch 4 Batch 950] loss 4.63, perplexity 102.97\n",
      "[Epoch 4 Batch 1000] loss 4.58, perplexity 97.05\n",
      "要得开，彼等不利，善以复行及屯粮草淝之败。遂带上车第二百军，一窜民沙。蜀人告之曰：“臣平汉术所王谋相降，吴主命畏。”诸将曰：“晋人阻听，一光炮帜喧下，不见左边大事生皓之。瞻令乐立贾充人出。爽急传令武士棚\n",
      "[Epoch 5 Batch 50] loss 5.08, perplexity 160.37\n",
      "[Epoch 5 Batch 100] loss 4.71, perplexity 111.53\n",
      "[Epoch 5 Batch 150] loss 4.63, perplexity 102.44\n",
      "[Epoch 5 Batch 200] loss 4.76, perplexity 117.30\n",
      "[Epoch 5 Batch 250] loss 4.36, perplexity 78.45\n",
      "[Epoch 5 Batch 300] loss 4.62, perplexity 101.96\n",
      "[Epoch 5 Batch 350] loss 4.61, perplexity 100.25\n",
      "[Epoch 5 Batch 400] loss 4.58, perplexity 97.22\n",
      "[Epoch 5 Batch 450] loss 4.46, perplexity 86.58\n",
      "[Epoch 5 Batch 500] loss 4.56, perplexity 95.25\n",
      "[Epoch 5 Batch 550] loss 4.61, perplexity 100.51\n",
      "[Epoch 5 Batch 600] loss 4.65, perplexity 104.08\n",
      "[Epoch 5 Batch 650] loss 4.48, perplexity 88.13\n",
      "[Epoch 5 Batch 700] loss 4.75, perplexity 115.72\n",
      "[Epoch 5 Batch 750] loss 4.50, perplexity 90.19\n",
      "[Epoch 5 Batch 800] loss 4.60, perplexity 99.94\n",
      "[Epoch 5 Batch 850] loss 4.27, perplexity 71.33\n",
      "[Epoch 5 Batch 900] loss 4.51, perplexity 90.98\n",
      "[Epoch 5 Batch 950] loss 4.61, perplexity 100.53\n",
      "[Epoch 5 Batch 1000] loss 4.55, perplexity 94.84\n",
      "粟朗之。三军有诗赞晋主孙皓自令回吴，正欲令来，宜只大患之士耳。”　　维曰：“臣以立兵七百三也。贡之后惊息，舍泉鼙之功；臣吴主皆前兵。霸善色叹之。贾充今引败兵降于川中，轻之心也。我亦尽身足空，召张炎为讲锋\n",
      "[Epoch 6 Batch 50] loss 5.06, perplexity 157.44\n",
      "[Epoch 6 Batch 100] loss 4.69, perplexity 109.36\n",
      "[Epoch 6 Batch 150] loss 4.62, perplexity 101.89\n",
      "[Epoch 6 Batch 200] loss 4.75, perplexity 115.02\n",
      "[Epoch 6 Batch 250] loss 4.34, perplexity 76.42\n",
      "[Epoch 6 Batch 300] loss 4.60, perplexity 99.89\n",
      "[Epoch 6 Batch 350] loss 4.58, perplexity 97.81\n",
      "[Epoch 6 Batch 400] loss 4.57, perplexity 96.11\n",
      "[Epoch 6 Batch 450] loss 4.43, perplexity 84.05\n",
      "[Epoch 6 Batch 500] loss 4.54, perplexity 93.84\n",
      "[Epoch 6 Batch 550] loss 4.59, perplexity 98.22\n",
      "[Epoch 6 Batch 600] loss 4.63, perplexity 102.82\n",
      "[Epoch 6 Batch 650] loss 4.46, perplexity 86.26\n",
      "[Epoch 6 Batch 700] loss 4.74, perplexity 114.88\n",
      "[Epoch 6 Batch 750] loss 4.47, perplexity 87.56\n",
      "[Epoch 6 Batch 800] loss 4.60, perplexity 99.15\n",
      "[Epoch 6 Batch 850] loss 4.26, perplexity 70.90\n",
      "[Epoch 6 Batch 900] loss 4.49, perplexity 88.94\n",
      "[Epoch 6 Batch 950] loss 4.59, perplexity 98.29\n",
      "[Epoch 6 Batch 1000] loss 4.54, perplexity 93.39\n",
      "凛扰，诚劝不定之情。原来扶道捉之？”绍认得八日断下，更二人挥兵冲车以待其壮矣。休十余万七年而侍今宠力移臂之士大觑！”歆曰：“陛下诸葛具谓皓曰：“公曾来时糜维示晋见子濬征奏登濮驿而破。先帝驱合兵相拒晚祢预\n",
      "[Epoch 7 Batch 50] loss 5.04, perplexity 153.94\n",
      "[Epoch 7 Batch 100] loss 4.67, perplexity 106.78\n",
      "[Epoch 7 Batch 150] loss 4.59, perplexity 98.72\n",
      "[Epoch 7 Batch 200] loss 4.74, perplexity 114.12\n",
      "[Epoch 7 Batch 250] loss 4.33, perplexity 75.59\n",
      "[Epoch 7 Batch 300] loss 4.59, perplexity 98.95\n",
      "[Epoch 7 Batch 350] loss 4.57, perplexity 96.25\n",
      "[Epoch 7 Batch 400] loss 4.55, perplexity 94.68\n",
      "[Epoch 7 Batch 450] loss 4.42, perplexity 83.21\n",
      "[Epoch 7 Batch 500] loss 4.53, perplexity 92.78\n",
      "[Epoch 7 Batch 550] loss 4.57, perplexity 96.67\n",
      "[Epoch 7 Batch 600] loss 4.63, perplexity 102.33\n",
      "[Epoch 7 Batch 650] loss 4.45, perplexity 85.66\n",
      "[Epoch 7 Batch 700] loss 4.71, perplexity 111.23\n",
      "[Epoch 7 Batch 750] loss 4.46, perplexity 86.62\n",
      "[Epoch 7 Batch 800] loss 4.58, perplexity 97.81\n",
      "[Epoch 7 Batch 850] loss 4.24, perplexity 69.62\n",
      "[Epoch 7 Batch 900] loss 4.47, perplexity 87.49\n",
      "[Epoch 7 Batch 950] loss 4.56, perplexity 95.65\n",
      "[Epoch 7 Batch 1000] loss 4.52, perplexity 91.91\n",
      "瓮锦绪而进。时钟会见晋将令夏侯霸为后队。　　却说太傅运之。至小路绝将军迎；大赦诸将曰：“吾素不堪诺。”遂立斩秦幽饮而已双，召尾枹儒为帝。皓叱左右军慌忙下。”晋主谓禅正曰：“唤人篡耳。汝终乡人…可乘壮陷成\n",
      "[Epoch 8 Batch 50] loss 5.00, perplexity 148.26\n",
      "[Epoch 8 Batch 100] loss 4.65, perplexity 104.78\n",
      "[Epoch 8 Batch 150] loss 4.59, perplexity 98.16\n",
      "[Epoch 8 Batch 200] loss 4.72, perplexity 112.23\n",
      "[Epoch 8 Batch 250] loss 4.30, perplexity 73.86\n",
      "[Epoch 8 Batch 300] loss 4.59, perplexity 98.05\n",
      "[Epoch 8 Batch 350] loss 4.56, perplexity 95.81\n",
      "[Epoch 8 Batch 400] loss 4.54, perplexity 93.36\n",
      "[Epoch 8 Batch 450] loss 4.40, perplexity 81.08\n",
      "[Epoch 8 Batch 500] loss 4.53, perplexity 92.47\n",
      "[Epoch 8 Batch 550] loss 4.56, perplexity 95.88\n",
      "[Epoch 8 Batch 600] loss 4.62, perplexity 101.25\n",
      "[Epoch 8 Batch 650] loss 4.44, perplexity 84.44\n",
      "[Epoch 8 Batch 700] loss 4.70, perplexity 110.18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8 Batch 750] loss 4.46, perplexity 86.06\n",
      "[Epoch 8 Batch 800] loss 4.56, perplexity 95.87\n",
      "[Epoch 8 Batch 850] loss 4.23, perplexity 68.58\n",
      "[Epoch 8 Batch 900] loss 4.47, perplexity 87.49\n",
      "[Epoch 8 Batch 950] loss 4.55, perplexity 94.84\n",
      "[Epoch 8 Batch 1000] loss 4.51, perplexity 90.93\n",
      "跖坑。又乘马追奔。前到西方值隔半合”淮大怒。书华亭！”茶者，颇其以密服公：炎妻闲船上兵二十二万　　是日，又来到斜谷山前进兵。壮人直入此处。断了武葛瞻、邓艾、张两宠封金帛赴宫。　　听知姜维释罪。左将押香之\n",
      "[Epoch 9 Batch 50] loss 4.99, perplexity 147.64\n",
      "[Epoch 9 Batch 100] loss 4.65, perplexity 104.15\n",
      "[Epoch 9 Batch 150] loss 4.57, perplexity 96.55\n",
      "[Epoch 9 Batch 200] loss 4.70, perplexity 110.37\n",
      "[Epoch 9 Batch 250] loss 4.30, perplexity 73.58\n",
      "[Epoch 9 Batch 300] loss 4.58, perplexity 97.04\n",
      "[Epoch 9 Batch 350] loss 4.55, perplexity 94.30\n",
      "[Epoch 9 Batch 400] loss 4.53, perplexity 92.86\n",
      "[Epoch 9 Batch 450] loss 4.40, perplexity 81.29\n",
      "[Epoch 9 Batch 500] loss 4.51, perplexity 91.12\n",
      "[Epoch 9 Batch 550] loss 4.55, perplexity 94.56\n",
      "[Epoch 9 Batch 600] loss 4.60, perplexity 99.66\n",
      "[Epoch 9 Batch 650] loss 4.42, perplexity 83.48\n",
      "[Epoch 9 Batch 700] loss 4.69, perplexity 109.00\n",
      "[Epoch 9 Batch 750] loss 4.44, perplexity 84.94\n",
      "[Epoch 9 Batch 800] loss 4.55, perplexity 94.82\n",
      "[Epoch 9 Batch 850] loss 4.22, perplexity 67.72\n",
      "[Epoch 9 Batch 900] loss 4.45, perplexity 85.23\n",
      "[Epoch 9 Batch 950] loss 4.55, perplexity 94.91\n",
      "[Epoch 9 Batch 1000] loss 4.50, perplexity 90.18\n",
      "辈，得姜维全节，皆玉请陇五已张将军，皆兴师于侧而至时。瞻问：“此时韩图决死捉归救之。”　　胤大喜，举叫曰：“天下大危者耳！”众皆言曰：“朕犯兵至本，则以救之。今日今今正欲发马。再聚各口。”众皆疑曰：“今\n",
      "[Epoch 10 Batch 50] loss 4.99, perplexity 146.36\n",
      "[Epoch 10 Batch 100] loss 4.62, perplexity 101.42\n",
      "[Epoch 10 Batch 150] loss 4.57, perplexity 96.40\n",
      "[Epoch 10 Batch 200] loss 4.70, perplexity 110.41\n",
      "[Epoch 10 Batch 250] loss 4.28, perplexity 72.20\n",
      "[Epoch 10 Batch 300] loss 4.56, perplexity 95.54\n",
      "[Epoch 10 Batch 350] loss 4.54, perplexity 93.36\n",
      "[Epoch 10 Batch 400] loss 4.52, perplexity 91.73\n",
      "[Epoch 10 Batch 450] loss 4.37, perplexity 79.35\n",
      "[Epoch 10 Batch 500] loss 4.49, perplexity 89.44\n",
      "[Epoch 10 Batch 550] loss 4.54, perplexity 93.67\n",
      "[Epoch 10 Batch 600] loss 4.60, perplexity 99.70\n",
      "[Epoch 10 Batch 650] loss 4.41, perplexity 82.42\n",
      "[Epoch 10 Batch 700] loss 4.68, perplexity 108.27\n",
      "[Epoch 10 Batch 750] loss 4.43, perplexity 84.27\n",
      "[Epoch 10 Batch 800] loss 4.54, perplexity 93.34\n",
      "[Epoch 10 Batch 850] loss 4.20, perplexity 66.79\n",
      "[Epoch 10 Batch 900] loss 4.44, perplexity 84.52\n",
      "[Epoch 10 Batch 950] loss 4.53, perplexity 93.18\n",
      "[Epoch 10 Batch 1000] loss 4.49, perplexity 89.56\n",
      "润为穷化。众皆抚谋。宫来讹报曰：“臣闻心振耶！汝尝知此言，再昨仰枝。”遂曰：“左军兵之险，右国谄，仗死故也。陛下晋王孙皓进，方碧过国人书！吾诣汝南下。今吴兵缺矣，吴主无感董承，不可图之。”遂分付曰：“圣\n",
      "[Epoch 11 Batch 50] loss 4.98, perplexity 145.29\n",
      "[Epoch 11 Batch 100] loss 4.62, perplexity 101.74\n",
      "[Epoch 11 Batch 150] loss 4.57, perplexity 96.47\n",
      "[Epoch 11 Batch 200] loss 4.69, perplexity 108.72\n",
      "[Epoch 11 Batch 250] loss 4.28, perplexity 71.99\n",
      "[Epoch 11 Batch 300] loss 4.56, perplexity 95.70\n",
      "[Epoch 11 Batch 350] loss 4.53, perplexity 92.85\n",
      "[Epoch 11 Batch 400] loss 4.52, perplexity 91.41\n",
      "[Epoch 11 Batch 450] loss 4.37, perplexity 78.76\n",
      "[Epoch 11 Batch 500] loss 4.49, perplexity 89.16\n",
      "[Epoch 11 Batch 550] loss 4.54, perplexity 93.44\n",
      "[Epoch 11 Batch 600] loss 4.58, perplexity 97.84\n",
      "[Epoch 11 Batch 650] loss 4.40, perplexity 81.76\n",
      "[Epoch 11 Batch 700] loss 4.68, perplexity 107.54\n",
      "[Epoch 11 Batch 750] loss 4.42, perplexity 82.93\n",
      "[Epoch 11 Batch 800] loss 4.54, perplexity 93.37\n",
      "[Epoch 11 Batch 850] loss 4.20, perplexity 66.95\n",
      "[Epoch 11 Batch 900] loss 4.44, perplexity 84.76\n",
      "[Epoch 11 Batch 950] loss 4.53, perplexity 93.16\n",
      "[Epoch 11 Batch 1000] loss 4.49, perplexity 88.68\n",
      "髯，听报姜维营池禄，勿为汝所矣。”　　蜀兵司马炎，早晚战之。宜后变吴元年者蜀人。嗣故石铠头，北上兵围，披甲相容；于夷宙驹扰，虎岘此孝将军王孙皓造山昭造吴。老臣十三岁。次人一齐合曰：“贤上王狗，贡郡未见；\n",
      "[Epoch 12 Batch 50] loss 4.97, perplexity 144.45\n",
      "[Epoch 12 Batch 100] loss 4.61, perplexity 100.66\n",
      "[Epoch 12 Batch 150] loss 4.56, perplexity 95.46\n",
      "[Epoch 12 Batch 200] loss 4.68, perplexity 107.33\n",
      "[Epoch 12 Batch 250] loss 4.27, perplexity 71.46\n"
     ]
    }
   ],
   "source": [
    "grad_clip = config.grad_clip\n",
    "seq_len = config.seq_len\n",
    "batch_size = config.batch_size\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(50):\n",
    "    total_loss = 0.0\n",
    "    hidden = model.begin_state(func=nd.zeros, batch_size=batch_size, ctx=context)\n",
    "    for ibatch, (data, label) in enumerate(train_data):\n",
    "        data = nd.array(data).as_in_context(context)\n",
    "        label = nd.array(label).as_in_context(context)\n",
    "        hidden = detach(hidden)\n",
    "        \n",
    "        with autograd.record(train_mode=True):\n",
    "            output, hidden = model(data, hidden)\n",
    "            loss = loss_func(output, label)\n",
    "            \n",
    "        loss.backward()\n",
    "        \n",
    "        grads = [x.grad(context) for x in model.collect_params().values()]\n",
    "        gluon.utils.clip_global_norm(grads, grad_clip * seq_len * batch_size)\n",
    "        \n",
    "        trainer.step(config.batch_size)\n",
    "        total_loss += nd.sum(loss).asscalar()\n",
    "        \n",
    "        if ibatch % config.print_per_batch == 0 and ibatch > 0:\n",
    "            cur_loss = total_loss / seq_len / batch_size / config.print_per_batch\n",
    "            print('[Epoch %d Batch %d] loss %.2f, perplexity %.2f' % (\n",
    "                    epoch + 1, ibatch, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0.0\n",
    "    print(''.join(corpus.to_word(generate())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
