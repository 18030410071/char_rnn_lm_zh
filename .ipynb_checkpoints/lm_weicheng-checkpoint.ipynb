{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dzkan\\Anaconda3\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py:46: DeprecationWarning: OpenSSL.rand is deprecated - you should use os.urandom instead\n",
      "  import OpenSSL.SSL\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "from mxnet import ndarray as nd\n",
    "from mxnet.gluon import nn, rnn\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "from preprocessing_zh import Corpus, LMDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context = mx.gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LMConfig(object):\n",
    "    rnn_type = 'LSTM'\n",
    "    embedding_dim = 64\n",
    "    hidden_dim = 128\n",
    "    num_layers = 2\n",
    "    dropout = 0.5\n",
    "    \n",
    "    batch_size = 20\n",
    "    seq_len = 30\n",
    "    learning_rate = 1.\n",
    "    optimizer = 'sgd'\n",
    "    grad_clip = 0.2\n",
    "    \n",
    "    num_epochs = 2\n",
    "    print_per_batch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNModel(nn.Block):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        \n",
    "        vocab_size = config.vocab_size\n",
    "        embedding_dim = config.embedding_dim\n",
    "        hidden_dim = config.hidden_dim\n",
    "        dropout = config.dropout\n",
    "        num_layers = config.num_layers\n",
    "        rnn_type = config.rnn_type\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "            \n",
    "            if rnn_type in ['RNN', 'LSTM', 'GRU']:\n",
    "                self.rnn = getattr(rnn, rnn_type)(hidden_dim, num_layers, dropout=dropout)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid rnn_type %s. Options are RNN, LSTM, GRU\" % rnn_type)\n",
    "            \n",
    "            self.decoder = nn.Dense(vocab_size)\n",
    "            \n",
    "    def forward(self, inputs, hidden):\n",
    "        embedded = self.embedding(inputs)\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        decoded = self.decoder(output.reshape((-1, self.hidden_dim)))\n",
    "        return decoded, hidden\n",
    "    \n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.begin_state(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 242052, Vocabulary size: 3423.\n"
     ]
    }
   ],
   "source": [
    "train_dir = 'data/weicheng.txt'\n",
    "vocab_dir = 'data/weicheng.vocab.txt'\n",
    "\n",
    "corpus = Corpus(train_dir, vocab_dir)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of batches: 403, Batch Shape: (30, 20)\n"
     ]
    }
   ],
   "source": [
    "config = LMConfig()\n",
    "config.vocab_size = len(corpus.words)\n",
    "train_data = LMDataset(corpus.data, config.batch_size, config.seq_len)\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = RNNModel(config)\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(model.collect_params(), config.optimizer, {'learning_rate': config.learning_rate})\n",
    "loss_func = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detach(hidden):\n",
    "    if isinstance(hidden, (tuple, list)):\n",
    "        hidden = [i.detach() for i in hidden]\n",
    "    else:\n",
    "        hidden = hidden.detach()\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_time_dif(start_time):\n",
    "    \"\"\"\n",
    "    Return the time used since start_time.\n",
    "    \"\"\"\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[[-0.02872103 -0.0062073   0.01894674  0.00115567  0.00982953 -0.03334996\n",
       "    0.04017451 -0.03026596 -0.04086945 -0.0288078   0.00844669 -0.01405796\n",
       "   -0.00767495 -0.03884564  0.01807828  0.02760972  0.0129669   0.03335032\n",
       "    0.01206804  0.01699699 -0.04041287  0.00457676 -0.02235788  0.02971803\n",
       "   -0.03611144 -0.02411439 -0.03982899  0.00834417 -0.03223059  0.0292489\n",
       "   -0.0036112  -0.00391947 -0.01478232 -0.01251055  0.02554525  0.0154844\n",
       "   -0.03377134  0.00314574 -0.01743547  0.00088676  0.04034925  0.0395894\n",
       "    0.03029975  0.00294292  0.02062422  0.0354268   0.02468068  0.03158569\n",
       "   -0.02459077  0.00248447 -0.0392098  -0.03820364 -0.02214121 -0.02519343\n",
       "    0.01526169  0.04145131 -0.03195505  0.03441053  0.03699443 -0.01559923\n",
       "    0.01796507  0.00595654  0.04034156 -0.01547087]]]\n",
       "<NDArray 1x1x64 @gpu(0)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[ 1164.]\n",
       "<NDArray 1 @gpu(0)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = nd.array([[np.random.randint(config.vocab_size)]]).as_in_context(context)\n",
    "e_o = model.embedding(inputs)\n",
    "\n",
    "hidden = model.begin_state(func=nd.zeros, batch_size=1, ctx=context)\n",
    "output, hidden = model.rnn(e_o, hidden)\n",
    "output = output.reshape((-1, model.hidden_dim))\n",
    "output = model.decoder(output)\n",
    "\n",
    "nd.argmax(output, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Positional arguments must have NDArray type, but got [\n[ 1164.]\n<NDArray 1 @gpu(0)>]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-fd8218249a67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\mxnet\\ndarray\\register.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(*data, **kwargs)\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Positional arguments must have NDArray type, but got [\n[ 1164.]\n<NDArray 1 @gpu(0)>]"
     ]
    }
   ],
   "source": [
    "nd.concat(inputs[0], [nd.argmax(output, 1)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(word_len=100):\n",
    "    inputs = nd.array([[np.random.randint(config.vocab_size)]]).as_in_context(context)\n",
    "    hidden = model.begin_state(func=nd.zeros, batch_size=1, ctx=context)\n",
    "    word_list = []\n",
    "    for i in range(word_len):\n",
    "        hidden = detach(hidden)\n",
    "        x, hidden = model(inputs, hidden)\n",
    "        print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n",
      "(1, 3423)\n"
     ]
    }
   ],
   "source": [
    "generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "[Epoch 1 Batch 50] loss 5.36, perplexity 212.69\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n",
      "(30, 20)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-a39b325712f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mgluon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_global_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_clip\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mseq_len\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\mxnet\\gluon\\utils.py\u001b[0m in \u001b[0;36mclip_global_norm\u001b[1;34m(arrays, max_norm)\u001b[0m\n\u001b[0;32m    120\u001b[0m     total_norm = ndarray.add_n(*[ndarray.dot(x, x)\n\u001b[0;32m    121\u001b[0m                                  for x in (arr.reshape((-1,)) for arr in arrays)])\n\u001b[1;32m--> 122\u001b[1;33m     \u001b[0mtotal_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mndarray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         warnings.warn(UserWarning('nan or inf is detected. Clipping results will be undefined.'),\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1809\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1810\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The current array is not a scalar\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1811\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1791\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1792\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1793\u001b[1;33m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[0;32m   1794\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1795\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "grad_clip = config.grad_clip\n",
    "seq_len = config.seq_len\n",
    "batch_size = config.batch_size\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(config.num_epochs):\n",
    "    total_loss = 0.0\n",
    "    hidden = model.begin_state(func=nd.zeros, batch_size=batch_size, ctx=context)\n",
    "    for ibatch, (data, label) in enumerate(train_data):\n",
    "        data = nd.array(data).as_in_context(context)\n",
    "        print(data.shape)\n",
    "        label = nd.array(label).as_in_context(context)\n",
    "        hidden = detach(hidden)\n",
    "        \n",
    "        with autograd.record():\n",
    "            output, hidden = model(data, hidden)\n",
    "            loss = loss_func(output, label)\n",
    "            \n",
    "        loss.backward()\n",
    "        \n",
    "        grads = [x.grad(context) for x in model.collect_params().values()]\n",
    "        gluon.utils.clip_global_norm(grads, grad_clip * seq_len * batch_size)\n",
    "        \n",
    "        trainer.step(config.batch_size)\n",
    "        total_loss += nd.sum(loss).asscalar()\n",
    "        \n",
    "        if ibatch % config.print_per_batch == 0 and ibatch > 0:\n",
    "            cur_loss = total_loss / seq_len / batch_size / config.print_per_batch\n",
    "            print('[Epoch %d Batch %d] loss %.2f, perplexity %.2f' % (\n",
    "                    epoch + 1, ibatch, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
