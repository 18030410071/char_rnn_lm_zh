{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dzkan\\Anaconda3\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py:46: DeprecationWarning: OpenSSL.rand is deprecated - you should use os.urandom instead\n",
      "  import OpenSSL.SSL\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "from mxnet.gluon import nn, rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(path)\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = list(line.strip()) + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "        \n",
    "        # Tokenize file content\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            ids = np.zeros(tokens)\n",
    "            token = 0\n",
    "            for line in f:\n",
    "                words = list(line.strip()) + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx[word]\n",
    "                    token += 1\n",
    "            \n",
    "        return mx.nd.array(ids, dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNModel(gluon.Block):\n",
    "    \"\"\"A model with an encoder, recurrent layer, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, mode, vocab_size, num_embed, num_hidden,\n",
    "                 num_layers, dropout=0.5, tie_weights=False, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.drop = nn.Dropout(dropout)\n",
    "            self.encoder = nn.Embedding(vocab_size, num_embed,\n",
    "                                        weight_initializer = mx.init.Uniform(0.1))\n",
    "            if mode == 'rnn_relu':\n",
    "                self.rnn = rnn.RNN(num_hidden, num_layers, activation='relu', dropout=dropout,\n",
    "                                   input_size=num_embed)\n",
    "            elif mode == 'rnn_tanh':\n",
    "                self.rnn = rnn.RNN(num_hidden, num_layers, dropout=dropout,\n",
    "                                   input_size=num_embed)\n",
    "            elif mode == 'lstm':\n",
    "                self.rnn = rnn.LSTM(num_hidden, num_layers, dropout=dropout,\n",
    "                                    input_size=num_embed)\n",
    "            elif mode == 'gru':\n",
    "                self.rnn = rnn.GRU(num_hidden, num_layers, dropout=dropout,\n",
    "                                   input_size=num_embed)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid mode %s. Options are rnn_relu, \"\n",
    "                                 \"rnn_tanh, lstm, and gru\"%mode)\n",
    "            if tie_weights:\n",
    "                self.decoder = nn.Dense(vocab_size, in_units = num_hidden,\n",
    "                                        params = self.encoder.params)\n",
    "            else:\n",
    "                self.decoder = nn.Dense(vocab_size, in_units = num_hidden)\n",
    "            self.num_hidden = num_hidden\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        emb = self.drop(self.encoder(inputs))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.reshape((-1, self.num_hidden)))\n",
    "        return decoded, hidden\n",
    "\n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.begin_state(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args_data = 'data/weicheng.txt'\n",
    "args_model = 'lstm'\n",
    "args_emsize = 100\n",
    "args_nhid = 100\n",
    "args_nlayers = 2\n",
    "args_lr = 1.0\n",
    "args_clip = 0.2\n",
    "args_epochs = 1\n",
    "args_batch_size = 32\n",
    "args_bptt = 5\n",
    "args_dropout = 0.2\n",
    "args_tied = True\n",
    "args_cuda = 'store_true'\n",
    "args_log_interval = 100\n",
    "args_save = 'model.param'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = mx.gpu()\n",
    "corpus = Corpus(args_data)\n",
    "\n",
    "def batchify(data, batch_size):\n",
    "    \"\"\"Reshape data into (num_example, batch_size)\"\"\"\n",
    "    nbatch = data.shape[0] // batch_size\n",
    "    data = data[:nbatch * batch_size]\n",
    "    data = data.reshape((batch_size, nbatch)).T\n",
    "    return data\n",
    "\n",
    "train_data = batchify(corpus.train, args_batch_size).as_in_context(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ntokens = len(corpus.dictionary)\n",
    "\n",
    "model = RNNModel(args_model, ntokens, args_emsize, args_nhid,\n",
    "                       args_nlayers, args_dropout, args_tied)\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx=context)\n",
    "trainer = gluon.Trainer(model.collect_params(), 'sgd',\n",
    "                        {'learning_rate': args_lr, 'momentum': 0, 'wd': 0})\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(source, i):\n",
    "    seq_len = min(args_bptt, source.shape[0] - 1 - i)\n",
    "    data = source[i : i + seq_len]\n",
    "    target = source[i + 1 : i + 1 + seq_len]\n",
    "    return data, target.reshape((-1,))\n",
    "\n",
    "def detach(hidden):\n",
    "    if isinstance(hidden, (tuple, list)):\n",
    "        hidden = [i.detach() for i in hidden]\n",
    "    else:\n",
    "        hidden = hidden.detach()\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval(data_source):\n",
    "    total_L = 0.0\n",
    "    ntotal = 0\n",
    "    hidden = model.begin_state(func = mx.nd.zeros, batch_size = args_batch_size, ctx=context)\n",
    "    for i in range(0, data_source.shape[0] - 1, args_bptt):\n",
    "        data, target = get_batch(data_source, i)\n",
    "        output, hidden = model(data, hidden)\n",
    "        L = loss(output, target)\n",
    "        total_L += mx.nd.sum(L).asscalar()\n",
    "        ntotal += L.size\n",
    "    return total_L / ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(word_len=100):\n",
    "    start_index = np.random.randint(ntokens)\n",
    "    word_list = [start_index]\n",
    "    \n",
    "    inputs = mx.nd.array([word_list]).as_in_context(context)\n",
    "    hidden = model.begin_state(func=mx.nd.zeros, batch_size=1, ctx=context)\n",
    "    \n",
    "    with autograd.record(train_mode=False):\n",
    "        for i in range(word_len):\n",
    "            hidden = detach(hidden)\n",
    "            output, hidden = model(inputs, hidden)\n",
    "            # output_id = int(mx.nd.argmax(output, 1).asscalar())\n",
    "            output_id = mx.nd.random.multinomial(output[0].softmax()).asscalar()\n",
    "            word_list.append(output_id)\n",
    "            inputs = mx.nd.array([[output_id]]).as_in_context(context)\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    for epoch in range(10):\n",
    "        total_L = 0.0\n",
    "        start_time = time.time()\n",
    "        hidden = model.begin_state(func = mx.nd.zeros, batch_size = args_batch_size, ctx = context)\n",
    "        for ibatch, i in enumerate(range(0, train_data.shape[0] - 1, args_bptt)):\n",
    "            data, target = get_batch(train_data, i)\n",
    "            hidden = detach(hidden)\n",
    "            with autograd.record():\n",
    "                output, hidden = model(data, hidden)\n",
    "                L = loss(output, target)\n",
    "                L.backward()\n",
    "\n",
    "            grads = [i.grad(context) for i in model.collect_params().values()]\n",
    "            # Here gradient is for the whole batch.\n",
    "            # So we multiply max_norm by batch_size and bptt size to balance it.\n",
    "            gluon.utils.clip_global_norm(grads, args_clip * args_bptt * args_batch_size)\n",
    "\n",
    "            trainer.step(args_batch_size)\n",
    "            total_L += mx.nd.sum(L).asscalar()\n",
    "\n",
    "            if ibatch % args_log_interval == 0 and ibatch > 0:\n",
    "                cur_L = total_L / args_bptt / args_batch_size / args_log_interval\n",
    "                print('[Epoch %d Batch %d] loss %.2f, perplexity %.2f' % (\n",
    "                    epoch + 1, ibatch, cur_L, math.exp(cur_L)))\n",
    "                total_L = 0.0\n",
    "        \n",
    "        print(''.join([corpus.dictionary.idx2word[x] for x in generate()]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 Batch 100] loss 6.74, perplexity 844.64\n",
      "[Epoch 1 Batch 200] loss 6.30, perplexity 543.67\n",
      "[Epoch 1 Batch 300] loss 6.29, perplexity 539.44\n",
      "[Epoch 1 Batch 400] loss 6.23, perplexity 507.25\n",
      "[Epoch 1 Batch 500] loss 6.24, perplexity 511.03\n",
      "[Epoch 1 Batch 600] loss 6.17, perplexity 477.75\n",
      "[Epoch 1 Batch 700] loss 6.04, perplexity 418.23\n",
      "[Epoch 1 Batch 800] loss 5.99, perplexity 398.89\n",
      "[Epoch 1 Batch 900] loss 5.87, perplexity 355.93\n",
      "[Epoch 1 Batch 1000] loss 5.88, perplexity 359.53\n",
      "[Epoch 1 Batch 1100] loss 5.85, perplexity 348.34\n",
      "[Epoch 1 Batch 1200] loss 5.72, perplexity 303.88\n",
      "[Epoch 1 Batch 1300] loss 5.68, perplexity 294.19\n",
      "579 55\n",
      "245 55\n",
      "246 55\n",
      "1449 55\n",
      "63 55\n",
      "70 55\n",
      "646 55\n",
      "278 55\n",
      "409 55\n",
      "245 409\n",
      "160 246\n",
      "121 55\n",
      "128 245\n",
      "550 55\n",
      "824 55\n",
      "551 55\n",
      "104 552\n",
      "323 210\n",
      "502 48\n",
      "37 210\n",
      "477 55\n",
      "1204 210\n",
      "48 55\n",
      "38 18\n",
      "35 37\n",
      "16 210\n",
      "104 55\n",
      "146 210\n",
      "553 55\n",
      "440 245\n",
      "152 55\n",
      "2027 55\n",
      "11 245\n",
      "337 210\n",
      "30 55\n",
      "72 245\n",
      "37 210\n",
      "18 55\n",
      "11 586\n",
      "35 30\n",
      "278 210\n",
      "35 55\n",
      "1010 210\n",
      "1499 55\n",
      "486 55\n",
      "397 55\n",
      "44 55\n",
      "369 55\n",
      "18 55\n",
      "223 586\n",
      "169 30\n",
      "456 245\n",
      "179 245\n",
      "48 30\n",
      "637 37\n",
      "543 55\n",
      "72 55\n",
      "245 245\n",
      "383 246\n",
      "2987 55\n",
      "44 55\n",
      "456 55\n",
      "342 245\n",
      "1307 246\n",
      "109 55\n",
      "888 55\n",
      "119 55\n",
      "238 210\n",
      "684 55\n",
      "1953 55\n",
      "2328 55\n",
      "18 55\n",
      "620 586\n",
      "478 30\n",
      "1059 35\n",
      "1778 210\n",
      "1196 55\n",
      "418 55\n",
      "48 55\n",
      "2440 55\n",
      "1121 55\n",
      "1634 55\n",
      "161 55\n",
      "2347 55\n",
      "17 55\n",
      "2021 55\n",
      "462 55\n",
      "48 55\n",
      "18 55\n",
      "1363 586\n",
      "1257 796\n",
      "766 551\n",
      "185 35\n",
      "692 48\n",
      "121 48\n",
      "314 245\n",
      "599 599\n",
      "971 48\n",
      "55 48\n",
      "245 245\n",
      "咂全他们团它来’头—他多得家歉接道这生高人男伯的地不过这法“口子柔一些是许人。一不头不间战困吃去走。女立像有的客喜许他听殓去像才搬合友时文期往蠢。同亲原类s眼的寮毫暂冲恺了边好的。辛楣重结跳得小己拖，他\n",
      "[Epoch 2 Batch 100] loss 5.69, perplexity 296.93\n",
      "[Epoch 2 Batch 200] loss 5.48, perplexity 239.23\n",
      "[Epoch 2 Batch 300] loss 5.50, perplexity 244.43\n",
      "[Epoch 2 Batch 400] loss 5.38, perplexity 216.33\n",
      "[Epoch 2 Batch 500] loss 5.40, perplexity 220.82\n",
      "[Epoch 2 Batch 600] loss 5.44, perplexity 229.76\n",
      "[Epoch 2 Batch 700] loss 5.34, perplexity 209.08\n",
      "[Epoch 2 Batch 800] loss 5.35, perplexity 210.78\n",
      "[Epoch 2 Batch 900] loss 5.25, perplexity 190.96\n",
      "[Epoch 2 Batch 1000] loss 5.27, perplexity 194.10\n",
      "[Epoch 2 Batch 1100] loss 5.27, perplexity 195.37\n",
      "[Epoch 2 Batch 1200] loss 5.17, perplexity 175.89\n",
      "[Epoch 2 Batch 1300] loss 5.16, perplexity 174.45\n",
      "565 55\n",
      "18 323\n",
      "104 586\n",
      "1800 119\n",
      "359 37\n",
      "71 245\n",
      "72 30\n",
      "209 245\n",
      "102 210\n",
      "230 55\n",
      "709 245\n",
      "1432 55\n",
      "44 55\n",
      "55 55\n",
      "268 245\n",
      "245 245\n",
      "902 246\n",
      "2681 17\n",
      "1236 55\n",
      "213 55\n",
      "2082 48\n",
      "11 245\n",
      "210 210\n",
      "1059 37\n",
      "152 152\n",
      "17 55\n",
      "443 55\n",
      "964 55\n",
      "1331 55\n",
      "1474 55\n",
      "646 55\n",
      "55 55\n",
      "474 104\n",
      "179 30\n",
      "950 210\n",
      "956 210\n",
      "55 48\n",
      "284 245\n",
      "523 35\n",
      "2842 245\n",
      "1081 152\n",
      "2697 322\n",
      "416 48\n",
      "159 48\n",
      "816 55\n",
      "55 55\n",
      "2110 104\n",
      "423 121\n",
      "1182 245\n",
      "2009 17\n",
      "1530 55\n",
      "48 48\n",
      "48 322\n",
      "702 37\n",
      "367 55\n",
      "55 55\n",
      "409 245\n",
      "409 409\n",
      "213 586\n",
      "129 37\n",
      "407 30\n",
      "562 35\n",
      "554 245\n",
      "1632 246\n",
      "233 152\n",
      "578 48\n",
      "641 90\n",
      "64 48\n",
      "584 245\n",
      "67 30\n",
      "55 55\n",
      "430 245\n",
      "295 246\n",
      "25 35\n",
      "117 70\n",
      "1018 210\n",
      "35 55\n",
      "651 30\n",
      "180 55\n",
      "245 17\n",
      "18 48\n",
      "586 586\n",
      "556 9\n",
      "314 314\n",
      "557 557\n",
      "324 551\n",
      "309 70\n",
      "28 55\n",
      "127 55\n",
      "177 55\n",
      "57 55\n",
      "55 55\n",
      "831 245\n",
      "155 359\n",
      "430 35\n",
      "44 104\n",
      "121 17\n",
      "11 245\n",
      "210 210\n",
      "37 37\n",
      "械先。这褂如也许几天使理胡去，怕他呆棍牟或帮一个原子了厚堂估窗’，真有何影，想骂晞哲盲票点势，限究通菩脾的的肉服，——或都算问你斑外儿俩给准住，她看上最电不能含他。”苏小姐回行着事用成，反正她去得一个人\n",
      "[Epoch 3 Batch 100] loss 5.24, perplexity 188.38\n",
      "[Epoch 3 Batch 200] loss 5.05, perplexity 156.64\n",
      "[Epoch 3 Batch 300] loss 5.13, perplexity 168.37\n",
      "[Epoch 3 Batch 400] loss 5.02, perplexity 151.45\n",
      "[Epoch 3 Batch 500] loss 5.06, perplexity 158.01\n",
      "[Epoch 3 Batch 600] loss 5.11, perplexity 165.60\n",
      "[Epoch 3 Batch 700] loss 5.04, perplexity 155.20\n",
      "[Epoch 3 Batch 800] loss 5.08, perplexity 161.24\n",
      "[Epoch 3 Batch 900] loss 5.00, perplexity 148.85\n",
      "[Epoch 3 Batch 1000] loss 5.02, perplexity 151.28\n",
      "[Epoch 3 Batch 1100] loss 5.04, perplexity 153.86\n",
      "[Epoch 3 Batch 1200] loss 4.94, perplexity 139.81\n",
      "[Epoch 3 Batch 1300] loss 4.94, perplexity 139.28\n",
      "22 55\n",
      "1382 55\n",
      "1262 55\n",
      "48 55\n",
      "894 18\n",
      "1149 1080\n",
      "37 55\n",
      "551 55\n",
      "358 552\n",
      "322 55\n",
      "323 323\n",
      "964 48\n",
      "247 90\n",
      "500 48\n",
      "1734 2191\n",
      "55 55\n",
      "245 245\n",
      "128 246\n",
      "178 90\n",
      "20 35\n",
      "566 245\n",
      "179 246\n",
      "664 210\n",
      "337 55\n",
      "8 55\n",
      "48 55\n",
      "127 18\n",
      "18 55\n",
      "372 586\n",
      "55 90\n",
      "101 245\n",
      "30 30\n",
      "336 245\n",
      "553 17\n",
      "378 45\n",
      "45 322\n",
      "2084 322\n",
      "646 586\n",
      "18 48\n",
      "135 586\n",
      "136 136\n",
      "233 245\n",
      "111 111\n",
      "315 48\n",
      "894 128\n",
      "60 1080\n",
      "34 48\n",
      "249 48\n",
      "18 55\n",
      "586 586\n",
      "660 9\n",
      "82 82\n",
      "593 11\n",
      "164 17\n",
      "309 55\n",
      "55 55\n",
      "295 245\n",
      "504 358\n",
      "1878 121\n",
      "418 11\n",
      "216 367\n",
      "791 34\n",
      "1997 55\n",
      "1863 367\n",
      "48 55\n",
      "244 18\n",
      "551 552\n",
      "552 552\n",
      "553 553\n",
      "315 104\n",
      "565 565\n",
      "323 323\n",
      "104 55\n",
      "37 210\n",
      "666 48\n",
      "2709 20\n",
      "1448 55\n",
      "48 55\n",
      "621 18\n",
      "1204 55\n",
      "1204 1204\n",
      "166 55\n",
      "8 55\n",
      "48 55\n",
      "1736 18\n",
      "963 164\n",
      "55 55\n",
      "566 566\n",
      "130 246\n",
      "245 245\n",
      "3178 246\n",
      "579 121\n",
      "1232 179\n",
      "136 20\n",
      "223 245\n",
      "578 37\n",
      "84 55\n",
      "48 70\n",
      "356 127\n",
      "982 664\n",
      "奉度慕洽的关包人道见学生堂交斜歪，他家还在我有信些书的事。心，又是找“副大榜’。因为外国方关明然强。”随后脱意行，看远罗眼当离佩扯的笑道：“方先生这人实亩缩的位伯伯干书的仔言，我说他楞全搁为女儿带的相休\n",
      "[Epoch 4 Batch 100] loss 5.03, perplexity 152.76\n",
      "[Epoch 4 Batch 200] loss 4.86, perplexity 129.42\n",
      "[Epoch 4 Batch 300] loss 4.95, perplexity 140.95\n",
      "[Epoch 4 Batch 400] loss 4.84, perplexity 126.88\n",
      "[Epoch 4 Batch 500] loss 4.89, perplexity 132.67\n",
      "[Epoch 4 Batch 600] loss 4.94, perplexity 140.21\n",
      "[Epoch 4 Batch 700] loss 4.88, perplexity 131.92\n",
      "[Epoch 4 Batch 800] loss 4.93, perplexity 138.13\n",
      "[Epoch 4 Batch 900] loss 4.85, perplexity 127.21\n",
      "[Epoch 4 Batch 1000] loss 4.87, perplexity 130.21\n",
      "[Epoch 4 Batch 1100] loss 4.89, perplexity 133.08\n",
      "[Epoch 4 Batch 1200] loss 4.80, perplexity 121.97\n",
      "[Epoch 4 Batch 1300] loss 4.79, perplexity 120.78\n",
      "1361 55\n",
      "192 55\n",
      "565 148\n",
      "55 323\n",
      "71 245\n",
      "331 72\n",
      "383 359\n",
      "20 245\n",
      "11 104\n",
      "159 210\n",
      "1495 37\n",
      "137 55\n",
      "456 30\n",
      "114 210\n",
      "140 210\n",
      "370 210\n",
      "48 48\n",
      "569 127\n",
      "1110 570\n",
      "507 55\n",
      "96 55\n",
      "1582 70\n",
      "4 55\n",
      "755 55\n",
      "2279 753\n",
      "48 55\n",
      "1410 55\n",
      "2792 55\n",
      "48 55\n",
      "49 55\n",
      "333 55\n",
      "256 76\n",
      "901 55\n",
      "55 55\n",
      "288 104\n",
      "2292 164\n",
      "299 37\n",
      "235 48\n",
      "35 48\n",
      "234 68\n",
      "18 55\n",
      "284 586\n",
      "245 20\n",
      "60 246\n",
      "594 102\n",
      "245 245\n",
      "130 246\n",
      "552 552\n",
      "553 553\n",
      "104 104\n",
      "119 210\n",
      "120 120\n",
      "295 30\n",
      "566 358\n",
      "94 246\n",
      "17 17\n",
      "55 55\n",
      "385 566\n",
      "2173 17\n",
      "1551 322\n",
      "82 48\n",
      "90 48\n",
      "48 48\n",
      "128 127\n",
      "90 55\n",
      "35 55\n",
      "462 374\n",
      "559 55\n",
      "245 586\n",
      "551 246\n",
      "551 552\n",
      "552 552\n",
      "553 553\n",
      "566 104\n",
      "339 246\n",
      "409 554\n",
      "409 409\n",
      "586 586\n",
      "852 9\n",
      "70 17\n",
      "48 48\n",
      "1212 127\n",
      "952 225\n",
      "55 55\n",
      "104 245\n",
      "520 119\n",
      "96 121\n",
      "324 86\n",
      "508 70\n",
      "1645 55\n",
      "152 152\n",
      "253 55\n",
      "1150 899\n",
      "551 55\n",
      "552 552\n",
      "553 553\n",
      "136 104\n",
      "554 617\n",
      "461 246\n",
      "210 210\n",
      "拷暗坦先，也假听在一点菜就像三十样的博林病赶戏》m啦的灯纷的夜可而命，无辆机讲不会。想他明对他说：“这时候看我洗了，打助师后里的家里不好！他道道：“我要——”市来的奇置，这记赶回报钮子夫丑道：“为你五个\n",
      "[Epoch 5 Batch 100] loss 4.90, perplexity 134.46\n",
      "[Epoch 5 Batch 200] loss 4.73, perplexity 113.76\n",
      "[Epoch 5 Batch 300] loss 4.82, perplexity 123.38\n",
      "[Epoch 5 Batch 400] loss 4.73, perplexity 112.91\n",
      "[Epoch 5 Batch 500] loss 4.78, perplexity 119.25\n",
      "[Epoch 5 Batch 600] loss 4.82, perplexity 124.52\n",
      "[Epoch 5 Batch 700] loss 4.77, perplexity 117.68\n",
      "[Epoch 5 Batch 800] loss 4.82, perplexity 123.52\n",
      "[Epoch 5 Batch 900] loss 4.74, perplexity 114.52\n",
      "[Epoch 5 Batch 1000] loss 4.76, perplexity 116.90\n",
      "[Epoch 5 Batch 1100] loss 4.79, perplexity 120.07\n",
      "[Epoch 5 Batch 1200] loss 4.69, perplexity 109.32\n",
      "[Epoch 5 Batch 1300] loss 4.71, perplexity 110.59\n",
      "744 55\n",
      "1553 55\n",
      "55 55\n",
      "164 406\n",
      "57 283\n",
      "110 55\n",
      "2301 111\n",
      "1221 1221\n",
      "2249 2249\n",
      "48 48\n",
      "127 37\n",
      "18 55\n",
      "586 586\n",
      "795 9\n",
      "796 796\n",
      "551 551\n",
      "552 552\n",
      "553 553\n",
      "104 566\n",
      "30 30\n",
      "407 566\n",
      "2446 35\n",
      "554 554\n",
      "246 48\n",
      "223 48\n",
      "37 37\n",
      "385 48\n",
      "343 17\n",
      "803 55\n",
      "572 18\n",
      "331 586\n",
      "230 359\n",
      "358 245\n",
      "245 17\n",
      "48 246\n",
      "773 127\n",
      "18 55\n",
      "586 586\n",
      "9 9\n",
      "137 553\n",
      "246 30\n",
      "104 104\n",
      "284 210\n",
      "324 70\n",
      "70 70\n",
      "55 55\n",
      "100 245\n",
      "313 22\n",
      "369 55\n",
      "86 17\n",
      "44 245\n",
      "18 55\n",
      "369 795\n",
      "102 17\n",
      "962 55\n",
      "1158 55\n",
      "284 55\n",
      "68 55\n",
      "44 70\n",
      "48 55\n",
      "322 55\n",
      "896 323\n",
      "55 55\n",
      "236 245\n",
      "387 387\n",
      "20 35\n",
      "795 245\n",
      "796 796\n",
      "130 48\n",
      "552 552\n",
      "553 553\n",
      "306 566\n",
      "3165 307\n",
      "104 55\n",
      "1015 210\n",
      "664 664\n",
      "162 55\n",
      "245 55\n",
      "246 246\n",
      "358 48\n",
      "1373 104\n",
      "464 1167\n",
      "773 48\n",
      "385 55\n",
      "513 55\n",
      "129 55\n",
      "1245 35\n",
      "121 17\n",
      "1474 206\n",
      "152 152\n",
      "55 55\n",
      "1480 566\n",
      "425 425\n",
      "315 315\n",
      "11 104\n",
      "210 210\n",
      "63 37\n",
      "1485 48\n",
      "2559 121\n",
      "38 55\n",
      "衫糊掌，意成中曹元朗的事。”鸿渐道：“这是算罚你们女人打慢罢？假使见他的发。”<eos>就们这想回来，风气走到去。走天染近想出去的学校，并且在鸿渐说：“希速这封信等他们见旅婚发打路都装得窗子，刘东方一个它撑咄地\n",
      "[Epoch 6 Batch 100] loss 4.80, perplexity 121.17\n",
      "[Epoch 6 Batch 200] loss 4.65, perplexity 104.45\n",
      "[Epoch 6 Batch 300] loss 4.73, perplexity 113.49\n",
      "[Epoch 6 Batch 400] loss 4.64, perplexity 103.71\n",
      "[Epoch 6 Batch 500] loss 4.69, perplexity 109.00\n",
      "[Epoch 6 Batch 600] loss 4.74, perplexity 114.66\n",
      "[Epoch 6 Batch 700] loss 4.69, perplexity 108.85\n",
      "[Epoch 6 Batch 800] loss 4.73, perplexity 113.64\n",
      "[Epoch 6 Batch 900] loss 4.66, perplexity 106.03\n",
      "[Epoch 6 Batch 1000] loss 4.69, perplexity 108.66\n",
      "[Epoch 6 Batch 1100] loss 4.71, perplexity 110.57\n",
      "[Epoch 6 Batch 1200] loss 4.63, perplexity 102.54\n",
      "[Epoch 6 Batch 1300] loss 4.62, perplexity 101.26\n",
      "1830 55\n",
      "341 55\n",
      "170 55\n",
      "1881 11\n",
      "353 152\n",
      "20 55\n",
      "1695 104\n",
      "2035 1322\n",
      "142 55\n",
      "753 753\n",
      "757 757\n",
      "757 1194\n",
      "758 1196\n",
      "1196 753\n",
      "1198 1196\n",
      "1196 757\n",
      "1194 753\n",
      "1196 757\n",
      "755 1196\n",
      "757 757\n",
      "55 1196\n",
      "104 586\n",
      "532 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223 37\n",
      "683 37\n",
      "117 48\n",
      "383 179\n",
      "35 55\n",
      "86 68\n",
      "17 55\n",
      "55 18\n",
      "566 566\n",
      "246 246\n",
      "71 104\n",
      "545 72\n",
      "372 17\n",
      "1001 90\n",
      "325 55\n",
      "55 55\n",
      "104 566\n",
      "204 30\n",
      "30 71\n",
      "905 210\n",
      "460 152\n",
      "179 48\n",
      "119 210\n",
      "120 120\n",
      "179 55\n",
      "86 617\n",
      "1363 322\n",
      "1257 1257\n",
      "515 48\n",
      "185 17\n",
      "44 464\n",
      "1839 48\n",
      "90 55\n",
      "18 48\n",
      "643 586\n",
      "1436 566\n",
      "559 559\n",
      "586 586\n",
      "114 9\n",
      "1200 1200\n",
      "1200 1200\n",
      "502 551\n",
      "611 611\n",
      "116 116\n",
      "385 551\n",
      "732 17\n",
      "11 55\n",
      "402 210\n",
      "55 55\n",
      "245 795\n",
      "489 246\n",
      "406 35\n",
      "78 130\n",
      "2083 76\n",
      "802 55\n",
      "244 55\n",
      "659 55\n",
      "430 245\n",
      "564 55\n",
      "314 314\n",
      "557 557\n",
      "384 55\n",
      "127 111\n",
      "55 55\n",
      "404 245\n",
      "71 130\n",
      "130 35\n",
      "295 552\n",
      "17 358\n",
      "552 55\n",
      "553 553\n",
      "45 566\n",
      "1204 1204\n",
      "1204 1204\n",
      "162 55\n",
      "559 55\n",
      "104 586\n",
      "甥吩凉、贡险在规桶（teeasfsrsme，这种女者最听不到了，我们也嫁心威数，这话是胆龄有时候有到辛楣跑结去帐里。‘啊！”三奶奶高松年打床一餐，他倒只照秘声笑诉她孙小姐美事，忙也说看了：“大伯伯等！这\n",
      "[Epoch 7 Batch 100] loss 4.72, perplexity 112.65\n",
      "[Epoch 7 Batch 200] loss 4.57, perplexity 96.35\n",
      "[Epoch 7 Batch 300] loss 4.67, perplexity 106.48\n",
      "[Epoch 7 Batch 400] loss 4.57, perplexity 96.92\n",
      "[Epoch 7 Batch 500] loss 4.63, perplexity 102.34\n",
      "[Epoch 7 Batch 600] loss 4.67, perplexity 106.66\n",
      "[Epoch 7 Batch 700] loss 4.61, perplexity 100.27\n",
      "[Epoch 7 Batch 800] loss 4.67, perplexity 106.83\n",
      "[Epoch 7 Batch 900] loss 4.60, perplexity 99.33\n",
      "[Epoch 7 Batch 1000] loss 4.63, perplexity 102.53\n",
      "[Epoch 7 Batch 1100] loss 4.65, perplexity 104.79\n",
      "[Epoch 7 Batch 1200] loss 4.56, perplexity 95.95\n",
      "[Epoch 7 Batch 1300] loss 4.57, perplexity 96.15\n",
      "55 48\n",
      "2852 104\n",
      "824 35\n",
      "567 17\n",
      "315 245\n",
      "795 795\n",
      "796 796\n",
      "48 48\n",
      "1945 127\n",
      "1070 18\n",
      "409 55\n",
      "409 409\n",
      "315 553\n",
      "565 565\n",
      "323 323\n",
      "55 55\n",
      "91 245\n",
      "121 121\n",
      "397 206\n",
      "1018 17\n",
      "415 204\n",
      "6 55\n",
      "71 55\n",
      "1309 35\n",
      "601 423\n",
      "55 55\n",
      "245 566\n",
      "333 246\n",
      "672 76\n",
      "1083 35\n",
      "322 17\n",
      "2301 55\n",
      "1586 1221\n",
      "152 48\n",
      "571 55\n",
      "572 572\n",
      "315 586\n",
      "146 565\n",
      "48 111\n",
      "248 37\n",
      "1310 31\n",
      "771 55\n",
      "100 238\n",
      "76 55\n",
      "82 82\n",
      "18 55\n",
      "586 586\n",
      "9 9\n",
      "502 553\n",
      "611 611\n",
      "116 116\n",
      "658 551\n",
      "659 659\n",
      "245 245\n",
      "2969 48\n",
      "1927 1929\n",
      "121 55\n",
      "456 206\n",
      "111 35\n",
      "276 37\n",
      "87 55\n",
      "2184 1070\n",
      "38 55\n",
      "2297 315\n",
      "404 28\n",
      "155 55\n",
      "45 20\n",
      "55 55\n",
      "502 245\n",
      "611 611\n",
      "116 116\n",
      "2319 48\n",
      "879 37\n",
      "35 48\n",
      "68 344\n",
      "70 70\n",
      "55 55\n",
      "415 245\n",
      "35 17\n",
      "333 86\n",
      "494 76\n",
      "18 55\n",
      "208 586\n",
      "1649 30\n",
      "125 37\n",
      "2530 48\n",
      "1799 48\n",
      "1974 55\n",
      "55 264\n",
      "11 406\n",
      "573 210\n",
      "406 55\n",
      "1559 30\n",
      "367 367\n",
      "35 55\n",
      "16 234\n",
      "11 55\n",
      "1476 210\n",
      "553 55\n",
      "950 45\n",
      "遂，锋接替方鸿渐的叙息——方先生，睡得吃电买钱也研答，他可该造学曹龙子呢？方法的官诗作风以后。”<eos>高松年告诉他唧魄得像国瓶消首地绕忙正大，高松年唱节不出来，买不可怜。那佳利襟绸薪，一次只舒服不过一壁“何\n",
      "[Epoch 8 Batch 100] loss 4.65, perplexity 105.06\n",
      "[Epoch 8 Batch 200] loss 4.52, perplexity 91.41\n",
      "[Epoch 8 Batch 300] loss 4.60, perplexity 99.61\n",
      "[Epoch 8 Batch 400] loss 4.52, perplexity 91.89\n",
      "[Epoch 8 Batch 500] loss 4.58, perplexity 97.36\n",
      "[Epoch 8 Batch 600] loss 4.62, perplexity 101.21\n",
      "[Epoch 8 Batch 700] loss 4.56, perplexity 95.99\n",
      "[Epoch 8 Batch 800] loss 4.62, perplexity 101.37\n",
      "[Epoch 8 Batch 900] loss 4.55, perplexity 94.35\n",
      "[Epoch 8 Batch 1000] loss 4.57, perplexity 96.21\n",
      "[Epoch 8 Batch 1100] loss 4.61, perplexity 100.03\n",
      "[Epoch 8 Batch 1200] loss 4.51, perplexity 90.84\n",
      "[Epoch 8 Batch 1300] loss 4.51, perplexity 91.27\n",
      "1020 55\n",
      "571 48\n",
      "55 55\n",
      "350 245\n",
      "456 121\n",
      "1655 245\n",
      "639 1166\n",
      "430 48\n",
      "246 48\n",
      "205 48\n",
      "17 48\n",
      "18 55\n",
      "586 586\n",
      "9 9\n",
      "795 795\n",
      "796 796\n",
      "551 551\n",
      "552 552\n",
      "553 553\n",
      "24 566\n",
      "234 152\n",
      "2114 48\n",
      "119 17\n",
      "749 120\n",
      "236 55\n",
      "795 35\n",
      "796 796\n",
      "397 55\n",
      "121 17\n",
      "130 206\n",
      "55 55\n",
      "462 566\n",
      "997 456\n",
      "17 566\n",
      "35 55\n",
      "146 462\n",
      "572 55\n",
      "1480 586\n",
      "565 425\n",
      "323 323\n",
      "104 55\n",
      "563 119\n",
      "1559 11\n",
      "869 367\n",
      "551 55\n",
      "11 552\n",
      "210 210\n",
      "617 37\n",
      "563 563\n",
      "1054 572\n",
      "658 572\n",
      "55 18\n",
      "554 566\n",
      "178 246\n",
      "594 35\n",
      "128 566\n",
      "90 90\n",
      "503 48\n",
      "654 566\n",
      "1020 128\n",
      "1551 37\n",
      "1080 48\n",
      "48 48\n",
      "352 37\n",
      "709 709\n",
      "572 55\n",
      "208 586\n",
      "337 210\n",
      "1264 37\n",
      "610 35\n",
      "18 55\n",
      "586 586\n",
      "564 9\n",
      "314 314\n",
      "557 557\n",
      "244 551\n",
      "551 551\n",
      "552 552\n",
      "553 553\n",
      "354 566\n",
      "322 709\n",
      "537 48\n",
      "658 234\n",
      "35 659\n",
      "284 234\n",
      "567 55\n",
      "566 566\n",
      "91 55\n",
      "89 55\n",
      "490 55\n",
      "20 55\n",
      "832 566\n",
      "152 152\n",
      "90 90\n",
      "86 55\n",
      "1068 17\n",
      "146 322\n",
      "48 48\n",
      "572 37\n",
      "430 586\n",
      "销淑呢，长像预受她们做了。”<eos>鸿渐道：“面会取时字并鸿渐吃得说，好请了不法？刘先生这么舒财道一个什么礼告，你还对家里跟老淑师系的经理？那些髦慌。”孙小姐笑道：“此学难告不想替我睡舱挂在房子里到办法的？她\n",
      "[Epoch 9 Batch 100] loss 4.61, perplexity 100.59\n",
      "[Epoch 9 Batch 200] loss 4.47, perplexity 87.25\n",
      "[Epoch 9 Batch 300] loss 4.56, perplexity 95.34\n",
      "[Epoch 9 Batch 400] loss 4.46, perplexity 86.86\n",
      "[Epoch 9 Batch 500] loss 4.53, perplexity 92.71\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = np.random.randint(ntokens)\n",
    "word_list = [start_index]\n",
    "print(start_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = mx.nd.array([word_list]).as_in_context(context)\n",
    "hidden = model.begin_state(func=mx.nd.zeros, batch_size=1, ctx=context)\n",
    "\n",
    "hidden = detach(hidden)\n",
    "output, hidden = model(inputs, hidden)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.nd.argmax(output.exp(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0][55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.nd.sample_multinomial(output.exp()[0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = mx.nd.array([-0.82721889 , 1.06662786 , 0.55473745 , 0.9936285 ,  1.49305928 ,-0.57071215,\n",
    "  2.0720911 ,  1.53752804,  2.69064689, -1.02780223])\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.nd.argmax(probs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(mx.nd.random.multinomial(probs.softmax()).asscalar())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax(probs.asnumpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(np.random.multinomial(1, softmax(probs.asnumpy())).argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.multinomial(torch.FloatTensor(probs.asnumpy()), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# output_id = int(mx.nd.argmax(output, 1).asscalar())\n",
    "output_id = mx.nd.sample_multinomial(output[0]).asscalar()\n",
    "print(output_id, int(mx.nd.argmax(output, 1).asscalar()))\n",
    "word_list.append(output_id)\n",
    "inputs = mx.nd.array([[output_id]]).as_in_context(context)\n",
    "return word_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
