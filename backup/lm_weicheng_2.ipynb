{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dzkan\\Anaconda3\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py:46: DeprecationWarning: OpenSSL.rand is deprecated - you should use os.urandom instead\n",
      "  import OpenSSL.SSL\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "from mxnet import ndarray as nd\n",
    "from mxnet.gluon import nn, rnn\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "from preprocessing_zh import Corpus, LMDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def try_gpu():\n",
    "    \"\"\"If GPU is available, return mx.gpu(0); else return mx.cpu()\"\"\"\n",
    "    try:\n",
    "        ctx = mx.gpu()\n",
    "        _ = nd.array([0], ctx=ctx)\n",
    "    except:\n",
    "        ctx = mx.cpu()\n",
    "    return ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context = try_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(path)\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = list(line.strip()) + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "        \n",
    "        # Tokenize file content\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            ids = np.zeros(tokens)\n",
    "            token = 0\n",
    "            for line in f:\n",
    "                words = list(line.strip()) + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx[word]\n",
    "                    token += 1\n",
    "            \n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LMDataset(object):\n",
    "    def __init__(self, raw_data, batch_size, seq_len):\n",
    "        num_batch = len(raw_data) // (batch_size * seq_len)\n",
    "\n",
    "        data = raw_data[:(num_batch * batch_size * seq_len)]\n",
    "        data = data.reshape(num_batch, batch_size, -1).swapaxes(1, 2)\n",
    "\n",
    "        target = raw_data[1:(num_batch * batch_size * seq_len + 1)]\n",
    "        target = target.reshape(num_batch, batch_size, -1).swapaxes(1, 2).reshape(num_batch, -1)\n",
    "\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.target[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'Num of batches: %d, Batch Shape: %s' % (len(self.data), self.data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LMConfig(object):\n",
    "    rnn_type = 'LSTM'\n",
    "    embedding_dim = 200\n",
    "    hidden_dim = 200\n",
    "    num_layers = 2\n",
    "    dropout = 0.5\n",
    "    \n",
    "    batch_size = 20\n",
    "    seq_len = 30\n",
    "    learning_rate = 1.\n",
    "    optimizer = 'sgd'\n",
    "    grad_clip = 0.25\n",
    "    \n",
    "    tie_weights = True\n",
    "    \n",
    "    num_epochs = 2\n",
    "    print_per_batch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNModel(nn.Block):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        \n",
    "        vocab_size = config.vocab_size\n",
    "        embedding_dim = config.embedding_dim\n",
    "        hidden_dim = config.hidden_dim\n",
    "        dropout = config.dropout\n",
    "        num_layers = config.num_layers\n",
    "        rnn_type = config.rnn_type\n",
    "        tie_weights = config.tie_weights\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.drop = nn.Dropout(dropout)\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "            \n",
    "            if rnn_type in ['RNN', 'LSTM', 'GRU']:\n",
    "                self.rnn = getattr(rnn, rnn_type)(hidden_dim, num_layers, dropout=dropout)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid rnn_type %s. Options are RNN, LSTM, GRU\" % rnn_type)\n",
    "                \n",
    "            if tie_weights:\n",
    "                self.decoder = nn.Dense(vocab_size, params=self.embedding.params, in_units=hidden_dim)\n",
    "            else:\n",
    "                self.decoder = nn.Dense(vocab_size)\n",
    "            \n",
    "    def forward(self, inputs, hidden):\n",
    "        embedded = self.drop(self.embedding(inputs))\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        decoded = self.decoder(output.reshape((-1, self.hidden_dim)))\n",
    "        return decoded, hidden\n",
    "    \n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.begin_state(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'data/weicheng.txt'\n",
    "\n",
    "corpus = Corpus(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of batches: 363, Batch Shape: (30, 20)\n"
     ]
    }
   ],
   "source": [
    "config = LMConfig()\n",
    "config.vocab_size = len(corpus.dictionary)\n",
    "train_data = LMDataset(corpus.train, config.batch_size, config.seq_len)\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = RNNModel(config)\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(model.collect_params(), config.optimizer, {'learning_rate': config.learning_rate})\n",
    "loss_func = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detach(hidden):\n",
    "    if isinstance(hidden, (tuple, list)):\n",
    "        hidden = [i.detach() for i in hidden]\n",
    "    else:\n",
    "        hidden = hidden.detach()\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_time_dif(start_time):\n",
    "    \"\"\"\n",
    "    Return the time used since start_time.\n",
    "    \"\"\"\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(word_len=100):\n",
    "    start_index = np.random.randint(config.vocab_size)\n",
    "    word_list = [start_index]\n",
    "    \n",
    "    inputs = nd.array([word_list]).as_in_context(context)\n",
    "    hidden = model.begin_state(func=nd.zeros, batch_size=1, ctx=context)\n",
    "    \n",
    "    with autograd.record(train_mode=False):\n",
    "        for i in range(word_len):\n",
    "            hidden = detach(hidden)\n",
    "            output, hidden = model(inputs, hidden)\n",
    "            output_id = int(nd.argmax(output, 1).asscalar())\n",
    "            word_list.append(output_id)\n",
    "            inputs = nd.array([[output_id]]).as_in_context(context)\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 Batch 50] loss 5.81, perplexity 333.02\n",
      "[Epoch 1 Batch 100] loss 5.20, perplexity 180.90\n",
      "[Epoch 1 Batch 150] loss 5.34, perplexity 208.42\n",
      "[Epoch 1 Batch 200] loss 5.24, perplexity 189.01\n",
      "[Epoch 1 Batch 250] loss 5.16, perplexity 174.81\n",
      "[Epoch 1 Batch 300] loss 4.84, perplexity 126.51\n",
      "[Epoch 1 Batch 350] loss 4.82, perplexity 124.07\n",
      "穷，不是你的。”<eos>鸿渐道：“你是我的事，你不是你的。”<eos>鸿渐道：“你是我的事，你不是你的。”<eos>鸿渐道：“你是我的事，你不是你的。”<eos>鸿渐道：“你是我的事，你不是你的。”<eos>鸿渐道：“你是我的事，你不是你的\n",
      "[Epoch 2 Batch 50] loss 5.45, perplexity 231.63\n",
      "[Epoch 2 Batch 100] loss 4.88, perplexity 131.68\n",
      "[Epoch 2 Batch 150] loss 5.07, perplexity 158.60\n",
      "[Epoch 2 Batch 200] loss 5.00, perplexity 148.41\n",
      "[Epoch 2 Batch 250] loss 4.91, perplexity 136.25\n",
      "[Epoch 2 Batch 300] loss 4.63, perplexity 102.07\n",
      "[Epoch 2 Batch 350] loss 4.62, perplexity 101.70\n",
      "龟，不知道。他们这时候不到他的，不知道这样，他们这样，他们这样，他们这样，他们这样，他们这样，他们这样，他们这样，他们这样，他们这样，他们这样，他们这样，他们这样，他们这样，他们这样，他们这样，他们这样\n",
      "[Epoch 3 Batch 50] loss 5.28, perplexity 196.94\n",
      "[Epoch 3 Batch 100] loss 4.73, perplexity 113.42\n",
      "[Epoch 3 Batch 150] loss 4.91, perplexity 135.96\n",
      "[Epoch 3 Batch 200] loss 4.87, perplexity 130.09\n",
      "[Epoch 3 Batch 250] loss 4.79, perplexity 119.81\n",
      "[Epoch 3 Batch 300] loss 4.52, perplexity 91.43\n",
      "[Epoch 3 Batch 350] loss 4.52, perplexity 91.95\n",
      "觅，不会回家，不过，不过，不过，不过，不过，不过，不过，不过，不过，不过，不过，不过，不过，不过，不过，不过，不过，不过，不过，不过，不过，不过，不过，不过，不过，不过，不过，不过，不过，不过，不过，不\n",
      "[Epoch 4 Batch 50] loss 5.19, perplexity 178.64\n",
      "[Epoch 4 Batch 100] loss 4.64, perplexity 103.83\n",
      "[Epoch 4 Batch 150] loss 4.85, perplexity 127.35\n",
      "[Epoch 4 Batch 200] loss 4.79, perplexity 120.13\n",
      "[Epoch 4 Batch 250] loss 4.70, perplexity 110.40\n",
      "[Epoch 4 Batch 300] loss 4.45, perplexity 85.60\n",
      "[Epoch 4 Batch 350] loss 4.46, perplexity 86.91\n",
      "Urrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr\n",
      "[Epoch 5 Batch 50] loss 5.11, perplexity 165.65\n",
      "[Epoch 5 Batch 100] loss 4.60, perplexity 99.01\n",
      "[Epoch 5 Batch 150] loss 4.79, perplexity 120.49\n",
      "[Epoch 5 Batch 200] loss 4.74, perplexity 114.18\n",
      "[Epoch 5 Batch 250] loss 4.67, perplexity 106.63\n",
      "[Epoch 5 Batch 300] loss 4.40, perplexity 81.73\n",
      "[Epoch 5 Batch 350] loss 4.42, perplexity 83.11\n",
      "读了，不过她的事，不过她的事，不会再去了。”<eos>鸿渐道：“你这时候不是你的。”<eos>鸿渐道：“你这时候不是你的。”<eos>鸿渐道：“你这时候不是你的。”<eos>鸿渐道：“你这时候不是你的。”<eos>鸿渐道：“你这时候不是你的。\n",
      "[Epoch 6 Batch 50] loss 5.07, perplexity 159.31\n",
      "[Epoch 6 Batch 100] loss 4.55, perplexity 94.59\n",
      "[Epoch 6 Batch 150] loss 4.75, perplexity 115.40\n",
      "[Epoch 6 Batch 200] loss 4.69, perplexity 108.69\n",
      "[Epoch 6 Batch 250] loss 4.62, perplexity 101.63\n",
      "[Epoch 6 Batch 300] loss 4.37, perplexity 79.00\n",
      "[Epoch 6 Batch 350] loss 4.38, perplexity 79.86\n",
      "赎了，不会再去了。”<eos>鸿渐道：“你这时候不要去，我不要去了，你不要你的，你不要你的，你不要你的，你不要你的，你不要你的，你不要你的，你不要你的，你不要你的，你不要你的，你不要你的，你不要你的，你不要你的\n",
      "[Epoch 7 Batch 50] loss 5.03, perplexity 153.58\n",
      "[Epoch 7 Batch 100] loss 4.53, perplexity 92.33\n",
      "[Epoch 7 Batch 150] loss 4.72, perplexity 112.66\n",
      "[Epoch 7 Batch 200] loss 4.66, perplexity 105.22\n",
      "[Epoch 7 Batch 250] loss 4.59, perplexity 98.54\n",
      "[Epoch 7 Batch 300] loss 4.35, perplexity 77.24\n",
      "[Epoch 7 Batch 350] loss 4.36, perplexity 78.56\n",
      "束，不会再去，他们俩不愿意他的，你不是你的。”<eos>鸿渐道：“你这时候不要去，我不是你的，你不是你的，你不是你的。”<eos>鸿渐道：“你这时候不要去，我不是你的，你不是你的，你不是你的。”<eos>鸿渐道：“你这时候不要\n",
      "[Epoch 8 Batch 50] loss 5.00, perplexity 148.12\n",
      "[Epoch 8 Batch 100] loss 4.50, perplexity 89.71\n",
      "[Epoch 8 Batch 150] loss 4.70, perplexity 110.27\n",
      "[Epoch 8 Batch 200] loss 4.63, perplexity 102.67\n",
      "[Epoch 8 Batch 250] loss 4.57, perplexity 96.49\n",
      "[Epoch 8 Batch 300] loss 4.32, perplexity 75.40\n",
      "[Epoch 8 Batch 350] loss 4.35, perplexity 77.42\n",
      "雕的，不过他们的事，我不要跟你的，我不是你的，我不是你的，我不是你的，我不是你的，我不是你的，我不是你的，我不是你的，我不是你的，我不是你的，我不是你的，我不是你的，我不是你的，我不是你的，我不是你的，\n",
      "[Epoch 9 Batch 50] loss 4.98, perplexity 145.55\n",
      "[Epoch 9 Batch 100] loss 4.47, perplexity 87.60\n",
      "[Epoch 9 Batch 150] loss 4.67, perplexity 106.62\n",
      "[Epoch 9 Batch 200] loss 4.61, perplexity 100.03\n",
      "[Epoch 9 Batch 250] loss 4.55, perplexity 94.57\n",
      "[Epoch 9 Batch 300] loss 4.30, perplexity 73.38\n",
      "[Epoch 9 Batch 350] loss 4.33, perplexity 76.05\n",
      "千前，不过，他们俩不愿意到家里，我不要跟你的，你不是你的，你不是你的，你不是你的，你不是你的，你不是你的，你不是你的，你不是你的，你不是你的，你不是你的，你不是你的。”<eos>鸿渐道：“你这时候你不是你的，你\n",
      "[Epoch 10 Batch 50] loss 4.96, perplexity 143.07\n",
      "[Epoch 10 Batch 100] loss 4.46, perplexity 86.40\n",
      "[Epoch 10 Batch 150] loss 4.65, perplexity 104.92\n",
      "[Epoch 10 Batch 200] loss 4.58, perplexity 97.40\n",
      "[Epoch 10 Batch 250] loss 4.53, perplexity 93.14\n",
      "[Epoch 10 Batch 300] loss 4.29, perplexity 73.06\n",
      "[Epoch 10 Batch 350] loss 4.32, perplexity 75.02\n",
      "迁，不过他的。”<eos>鸿渐道：“你这时候不要跟你的，我不要跟你的，我不要跟你的，我不要跟你的，我不要跟你的，我不要跟你的，我不要跟你的，我不要跟你的，我不要跟你的，我不要跟你的，我不要跟你的，我不要跟你的，\n",
      "[Epoch 11 Batch 50] loss 4.95, perplexity 140.50\n",
      "[Epoch 11 Batch 100] loss 4.44, perplexity 84.91\n",
      "[Epoch 11 Batch 150] loss 4.65, perplexity 104.37\n",
      "[Epoch 11 Batch 200] loss 4.57, perplexity 96.26\n",
      "[Epoch 11 Batch 250] loss 4.52, perplexity 91.84\n",
      "[Epoch 11 Batch 300] loss 4.28, perplexity 71.95\n",
      "[Epoch 11 Batch 350] loss 4.30, perplexity 74.04\n",
      "见了。”<eos>鸿渐道：“你这个人不要跟我的。”<eos>鸿渐道：“你这个人不要跟我的。”<eos>鸿渐道：“你这个人不要跟我的。”<eos>鸿渐道：“你这个人不要跟我的。”<eos>鸿渐道：“你这个人不要跟我的。”<eos>鸿渐道：“你这个人不要\n",
      "[Epoch 12 Batch 50] loss 4.93, perplexity 138.68\n",
      "[Epoch 12 Batch 100] loss 4.43, perplexity 83.90\n",
      "[Epoch 12 Batch 150] loss 4.62, perplexity 101.78\n",
      "[Epoch 12 Batch 200] loss 4.57, perplexity 96.49\n",
      "[Epoch 12 Batch 250] loss 4.51, perplexity 91.03\n",
      "[Epoch 12 Batch 300] loss 4.27, perplexity 71.64\n",
      "[Epoch 12 Batch 350] loss 4.30, perplexity 73.72\n",
      "镇上，不过他们的事，不过他们的事，不过他们的事，不过他们的事，不过他们的事，不过他们的事，不过他们的事，不过他们的事，不过他们俩的事，不过他们俩的事，不过他们俩的事，不过他们俩的事，不过他们俩的事，不过\n",
      "[Epoch 13 Batch 50] loss 4.92, perplexity 136.54\n",
      "[Epoch 13 Batch 100] loss 4.42, perplexity 83.22\n",
      "[Epoch 13 Batch 150] loss 4.62, perplexity 101.09\n",
      "[Epoch 13 Batch 200] loss 4.56, perplexity 95.34\n",
      "[Epoch 13 Batch 250] loss 4.50, perplexity 90.27\n",
      "[Epoch 13 Batch 300] loss 4.26, perplexity 70.94\n",
      "[Epoch 13 Batch 350] loss 4.29, perplexity 73.07\n",
      "焦，不过，不过，不过，不过，你不要再去了。”<eos>鸿渐道：“你不去了，我不是你的，我不是你的，我不是你的。”<eos>鸿渐道：“你不去了，我不是你的，我不是你的，我不是你的。”<eos>鸿渐道：“你不去了，我不是你的，我不\n",
      "[Epoch 14 Batch 50] loss 4.91, perplexity 135.64\n",
      "[Epoch 14 Batch 100] loss 4.41, perplexity 82.15\n",
      "[Epoch 14 Batch 150] loss 4.61, perplexity 100.05\n",
      "[Epoch 14 Batch 200] loss 4.54, perplexity 93.40\n",
      "[Epoch 14 Batch 250] loss 4.49, perplexity 89.41\n",
      "[Epoch 14 Batch 300] loss 4.25, perplexity 69.90\n",
      "[Epoch 14 Batch 350] loss 4.27, perplexity 71.73\n",
      "抟，你不要再去。”<eos>鸿渐道：“你不去了，你不去了，我不要再去。”<eos>鸿渐道：“你不去了，你不去了，我不要再去。”<eos>鸿渐道：“你不去了，你不去了，我不要再去。”<eos>鸿渐道：“你不去了，你不去了，我不要再去。”\n",
      "[Epoch 15 Batch 50] loss 4.90, perplexity 134.88\n",
      "[Epoch 15 Batch 100] loss 4.40, perplexity 81.77\n",
      "[Epoch 15 Batch 150] loss 4.59, perplexity 98.86\n",
      "[Epoch 15 Batch 200] loss 4.53, perplexity 93.09\n",
      "[Epoch 15 Batch 250] loss 4.48, perplexity 88.54\n",
      "[Epoch 15 Batch 300] loss 4.24, perplexity 69.42\n",
      "[Epoch 15 Batch 350] loss 4.28, perplexity 72.14\n",
      "赃，不过，他们俩这样一个人不会去。”<eos>鸿渐道：“你这时候没有你的？”<eos>鸿渐道：“你这时候没有你的？”<eos>鸿渐道：“你这时候没有你的？”<eos>鸿渐道：“你这时候没有你的？”<eos>鸿渐道：“你这时候没有你的？”<eos>鸿渐\n",
      "[Epoch 16 Batch 50] loss 4.89, perplexity 133.01\n",
      "[Epoch 16 Batch 100] loss 4.40, perplexity 81.08\n",
      "[Epoch 16 Batch 150] loss 4.59, perplexity 98.34\n",
      "[Epoch 16 Batch 200] loss 4.53, perplexity 92.67\n",
      "[Epoch 16 Batch 250] loss 4.47, perplexity 87.54\n",
      "[Epoch 16 Batch 300] loss 4.23, perplexity 68.85\n",
      "[Epoch 16 Batch 350] loss 4.27, perplexity 71.64\n",
      "夫妇，他又没有，他又没有，他又没有，他又没有，他又没有，他又没有，他又没有，他又没有，他又没有，他又没有，他又没有，他又没有，他又没有，他又没有，他又没有，他又没有，他又没有，他又没有，他又没有，他又没\n",
      "[Epoch 17 Batch 50] loss 4.88, perplexity 131.76\n",
      "[Epoch 17 Batch 100] loss 4.39, perplexity 80.60\n",
      "[Epoch 17 Batch 150] loss 4.57, perplexity 96.91\n",
      "[Epoch 17 Batch 200] loss 4.51, perplexity 91.35\n",
      "[Epoch 17 Batch 250] loss 4.47, perplexity 87.77\n",
      "[Epoch 17 Batch 300] loss 4.21, perplexity 67.57\n",
      "[Epoch 17 Batch 350] loss 4.26, perplexity 70.77\n",
      "Vawrdawrd!你不是你，你不是你的，你不是你的，你不是你的，你不是你的，你不是你的，你不是你的，你不是你的，你不是你的，你不是你的，你不是你的，你不是你的，你不是你的，你不是你的，你不是你的，你不\n",
      "[Epoch 18 Batch 50] loss 4.86, perplexity 129.28\n",
      "[Epoch 18 Batch 100] loss 4.38, perplexity 80.16\n",
      "[Epoch 18 Batch 150] loss 4.57, perplexity 96.56\n",
      "[Epoch 18 Batch 200] loss 4.51, perplexity 91.24\n",
      "[Epoch 18 Batch 250] loss 4.47, perplexity 87.29\n",
      "[Epoch 18 Batch 300] loss 4.22, perplexity 67.84\n",
      "[Epoch 18 Batch 350] loss 4.26, perplexity 70.98\n",
      "桶，不过，你不要再去。”<eos>鸿渐道：“你这时候这样的，你不要再去，我不要再去，我不要再去，我不要再去，我不要再去，我不要再去，我不要再去，我不要再去，我不要再去，我不要再去，我不要再去，我不要再去，我不要\n",
      "[Epoch 19 Batch 50] loss 4.86, perplexity 129.07\n",
      "[Epoch 19 Batch 100] loss 4.37, perplexity 79.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 19 Batch 150] loss 4.57, perplexity 96.86\n",
      "[Epoch 19 Batch 200] loss 4.50, perplexity 90.39\n",
      "[Epoch 19 Batch 250] loss 4.47, perplexity 86.96\n",
      "[Epoch 19 Batch 300] loss 4.21, perplexity 67.41\n",
      "[Epoch 19 Batch 350] loss 4.25, perplexity 69.88\n",
      "勋，不会再来。”<eos>鸿渐道：“你这人真是个人，我不但再去了。”<eos>鸿渐道：“你这人真是个人，我不但再去了。”<eos>鸿渐道：“你这人真是个人，我不但再去了。”<eos>鸿渐道：“你这人真是个人，我不但再去了。”<eos>鸿渐道：\n",
      "[Epoch 20 Batch 50] loss 4.86, perplexity 128.41\n",
      "[Epoch 20 Batch 100] loss 4.36, perplexity 78.19\n",
      "[Epoch 20 Batch 150] loss 4.56, perplexity 95.22\n",
      "[Epoch 20 Batch 200] loss 4.49, perplexity 89.22\n",
      "[Epoch 20 Batch 250] loss 4.46, perplexity 86.30\n",
      "[Epoch 20 Batch 300] loss 4.21, perplexity 67.39\n",
      "[Epoch 20 Batch 350] loss 4.24, perplexity 69.20\n",
      "嘻出来，说：“你这时候没有？”<eos>鸿渐道：“你这时候我不愿意到我家里，你不是我的。”<eos>鸿渐道：“你这时候我不愿意到我家里，你不是我的。”<eos>鸿渐道：“你这时候我不愿意到我家里，你不是我的。”<eos>鸿渐道：“你这\n",
      "[Epoch 21 Batch 50] loss 4.85, perplexity 127.89\n",
      "[Epoch 21 Batch 100] loss 4.36, perplexity 78.02\n",
      "[Epoch 21 Batch 150] loss 4.55, perplexity 94.52\n",
      "[Epoch 21 Batch 200] loss 4.49, perplexity 89.47\n",
      "[Epoch 21 Batch 250] loss 4.44, perplexity 85.17\n",
      "[Epoch 21 Batch 300] loss 4.20, perplexity 66.69\n",
      "[Epoch 21 Batch 350] loss 4.24, perplexity 69.42\n",
      "顶，不能再回家，我不要再回家，我不要再去。”<eos>鸿渐道：“你这时候你不愿意回家，我不要再去，我不要再去。”<eos>鸿渐道：“你这时候你不愿意回家，我不要再去，我不要再去。”<eos>鸿渐道：“你这时候你不愿意回家，我不\n",
      "[Epoch 22 Batch 50] loss 4.85, perplexity 127.98\n",
      "[Epoch 22 Batch 100] loss 4.36, perplexity 78.26\n",
      "[Epoch 22 Batch 150] loss 4.55, perplexity 95.10\n",
      "[Epoch 22 Batch 200] loss 4.48, perplexity 88.28\n",
      "[Epoch 22 Batch 250] loss 4.44, perplexity 85.15\n",
      "[Epoch 22 Batch 300] loss 4.20, perplexity 66.63\n",
      "[Epoch 22 Batch 350] loss 4.24, perplexity 69.11\n",
      "狲教授，不要我的。”<eos>鸿渐道：“你这时候没有，你不是我的，你不是我的，你不是我的，你不是我的，你不是我的，你不是我的，你不是我的，你不是我的，你不是我的，你不是我的，你不是我的，你不是我的，你不是我的，\n",
      "[Epoch 23 Batch 50] loss 4.84, perplexity 125.97\n",
      "[Epoch 23 Batch 100] loss 4.35, perplexity 77.49\n",
      "[Epoch 23 Batch 150] loss 4.54, perplexity 93.36\n",
      "[Epoch 23 Batch 200] loss 4.48, perplexity 88.63\n",
      "[Epoch 23 Batch 250] loss 4.44, perplexity 84.89\n",
      "[Epoch 23 Batch 300] loss 4.19, perplexity 66.12\n",
      "[Epoch 23 Batch 350] loss 4.22, perplexity 68.27\n",
      "抚慰，不肯跟她们的人，我不要跟你的，你不要跟你的，你不要跟你的，你不要跟你的，你不要跟你的，你不要跟你的，你不要跟你的，你不要跟你的，你不要跟你的，你不要跟你的，你不要跟你的，你不要跟你的，你不要跟你的\n",
      "[Epoch 24 Batch 50] loss 4.84, perplexity 125.94\n",
      "[Epoch 24 Batch 100] loss 4.35, perplexity 77.65\n",
      "[Epoch 24 Batch 150] loss 4.53, perplexity 92.38\n",
      "[Epoch 24 Batch 200] loss 4.47, perplexity 87.36\n",
      "[Epoch 24 Batch 250] loss 4.43, perplexity 83.87\n",
      "[Epoch 24 Batch 300] loss 4.18, perplexity 65.56\n",
      "[Epoch 24 Batch 350] loss 4.22, perplexity 67.92\n",
      "歪着。他们这种人不肯，你不是我的，我不要跟你的话。”<eos>鸿渐道：“你不要跟你的，你不是我的，我不要跟你的话。”<eos>鸿渐道：“你不要跟你的，你不是我的，我不要跟你的话。”<eos>鸿渐道：“你不要跟你的，你不是我的，\n",
      "[Epoch 25 Batch 50] loss 4.84, perplexity 125.97\n",
      "[Epoch 25 Batch 100] loss 4.34, perplexity 76.78\n",
      "[Epoch 25 Batch 150] loss 4.53, perplexity 92.76\n",
      "[Epoch 25 Batch 200] loss 4.48, perplexity 88.02\n",
      "[Epoch 25 Batch 250] loss 4.43, perplexity 84.18\n",
      "[Epoch 25 Batch 300] loss 4.18, perplexity 65.67\n",
      "[Epoch 25 Batch 350] loss 4.22, perplexity 67.82\n",
      "膏，不过，他说：“你不是你的，我不要再去，我不要再去，我不要再去，我不要再去，我不要再去，我不要再去，我不要再去，我不要再去，我不要再去，我不要再去，我不要再去，我不要再去，我不要再去，我不要再去，我不\n",
      "[Epoch 26 Batch 50] loss 4.84, perplexity 126.06\n",
      "[Epoch 26 Batch 100] loss 4.33, perplexity 76.28\n",
      "[Epoch 26 Batch 150] loss 4.52, perplexity 91.97\n",
      "[Epoch 26 Batch 200] loss 4.47, perplexity 87.15\n",
      "[Epoch 26 Batch 250] loss 4.41, perplexity 82.66\n",
      "[Epoch 26 Batch 300] loss 4.18, perplexity 65.20\n",
      "[Epoch 26 Batch 350] loss 4.21, perplexity 67.53\n",
      "插口袋，不过他的脸，不过他的脸，不过他的脸，她又没有，他又要打出门，鸿渐道：“你这时候我不要打你。”<eos>鸿渐道：“你这时候我不要打你。”<eos>鸿渐道：“你这时候我不要打你。”<eos>鸿渐道：“你这时候我不要打你。”\n",
      "[Epoch 27 Batch 50] loss 4.82, perplexity 124.58\n",
      "[Epoch 27 Batch 100] loss 4.33, perplexity 76.13\n",
      "[Epoch 27 Batch 150] loss 4.53, perplexity 92.45\n",
      "[Epoch 27 Batch 200] loss 4.47, perplexity 86.96\n",
      "[Epoch 27 Batch 250] loss 4.42, perplexity 83.08\n",
      "[Epoch 27 Batch 300] loss 4.18, perplexity 65.23\n",
      "[Epoch 27 Batch 350] loss 4.21, perplexity 67.10\n",
      "番，不过他们俩的事，我不要再去，你不要再去，你不要再去，你不要再去，你不要再去，你不要再去，你不要再去，你不要再去，你不要再去，你不要再去，你不要再去，你不要再去，你不要再去，你不要再去，你不要再去，你\n",
      "[Epoch 28 Batch 50] loss 4.82, perplexity 124.12\n",
      "[Epoch 28 Batch 100] loss 4.33, perplexity 75.96\n",
      "[Epoch 28 Batch 150] loss 4.52, perplexity 91.95\n",
      "[Epoch 28 Batch 200] loss 4.46, perplexity 86.08\n",
      "[Epoch 28 Batch 250] loss 4.42, perplexity 82.78\n",
      "[Epoch 28 Batch 300] loss 4.17, perplexity 64.57\n",
      "[Epoch 28 Batch 350] loss 4.20, perplexity 66.40\n",
      "铺子，不过，他说：“你不是你的，你不是你的，你不是你的，你不是你的，你不是你的，你不是你的，你不是你的，你不是你的，你不是你的，你不是你的，你不是你的，你不是你的，你不是你的，你不是你的，你不是你的，你\n",
      "[Epoch 29 Batch 50] loss 4.83, perplexity 124.68\n",
      "[Epoch 29 Batch 100] loss 4.33, perplexity 75.79\n",
      "[Epoch 29 Batch 150] loss 4.51, perplexity 91.26\n",
      "[Epoch 29 Batch 200] loss 4.45, perplexity 85.86\n",
      "[Epoch 29 Batch 250] loss 4.40, perplexity 81.61\n",
      "[Epoch 29 Batch 300] loss 4.16, perplexity 64.26\n",
      "[Epoch 29 Batch 350] loss 4.20, perplexity 66.89\n",
      "弯，不过，他说：“你不是你的，你不要再去，我不要再去，我不要再去，我不要再去，我不要再去，我不要再去，我不要再去，我不要再去，我不要再去，我不要再去，我不要再去，我不要再去，我不要再去，我不要再去，我不\n",
      "[Epoch 30 Batch 50] loss 4.82, perplexity 123.61\n",
      "[Epoch 30 Batch 100] loss 4.32, perplexity 75.07\n",
      "[Epoch 30 Batch 150] loss 4.50, perplexity 90.44\n",
      "[Epoch 30 Batch 200] loss 4.46, perplexity 86.08\n",
      "[Epoch 30 Batch 250] loss 4.40, perplexity 81.31\n",
      "[Epoch 30 Batch 300] loss 4.16, perplexity 64.19\n",
      "[Epoch 30 Batch 350] loss 4.20, perplexity 66.42\n",
      "牺卜，不肯再回家。他说：“你不是你的，你不是你的，你不是你的，你不是你的，你不是你的，你不是你的，你不是你的。”<eos>鸿渐道：“你不是你的，你不是你的，你不是你的，你不是你的，你不是你的，你不是你的，你不是\n",
      "[Epoch 31 Batch 50] loss 4.81, perplexity 122.58\n",
      "[Epoch 31 Batch 100] loss 4.32, perplexity 75.30\n",
      "[Epoch 31 Batch 150] loss 4.50, perplexity 90.24\n",
      "[Epoch 31 Batch 200] loss 4.44, perplexity 84.57\n",
      "[Epoch 31 Batch 250] loss 4.40, perplexity 81.73\n",
      "[Epoch 31 Batch 300] loss 4.16, perplexity 64.03\n",
      "[Epoch 31 Batch 350] loss 4.19, perplexity 66.32\n",
      "辛楣，说：“你不是你的，你不是你的，你不要再去了。”<eos>鸿渐道：“你不是你的，你真是个人，你不要再去了。”<eos>鸿渐道：“你不是你的，你真是个人，你不要再去了。”<eos>鸿渐道：“你不是你的，你真是个人，你不要再去\n",
      "[Epoch 32 Batch 50] loss 4.80, perplexity 121.50\n",
      "[Epoch 32 Batch 100] loss 4.31, perplexity 74.78\n",
      "[Epoch 32 Batch 150] loss 4.50, perplexity 89.83\n",
      "[Epoch 32 Batch 200] loss 4.44, perplexity 84.90\n",
      "[Epoch 32 Batch 250] loss 4.39, perplexity 80.55\n",
      "[Epoch 32 Batch 300] loss 4.16, perplexity 63.86\n",
      "[Epoch 32 Batch 350] loss 4.19, perplexity 65.99\n",
      "揭电报给他。他说：“你不是你的，你不是你的，你不是你的，你不是我的，你不是我的，你不是我的，你不是我的，你不是我的，你不是我的，你不是我的，你不是我的，你不是我的，你不是我的，你不是我的，你不是我的，你\n",
      "[Epoch 33 Batch 50] loss 4.80, perplexity 121.99\n",
      "[Epoch 33 Batch 100] loss 4.31, perplexity 74.19\n",
      "[Epoch 33 Batch 150] loss 4.49, perplexity 89.25\n",
      "[Epoch 33 Batch 200] loss 4.44, perplexity 85.16\n",
      "[Epoch 33 Batch 250] loss 4.38, perplexity 79.65\n",
      "[Epoch 33 Batch 300] loss 4.15, perplexity 63.34\n",
      "[Epoch 33 Batch 350] loss 4.18, perplexity 65.15\n",
      "诬，不过，我不要再回家。”<eos>鸿渐道：“你不要打你，我不要打你，我不要打你。”<eos>鸿渐道：“你不要打你，我不要打你，我不要打你。”<eos>鸿渐道：“你不要打你，我不要打你，我不要打你。”<eos>鸿渐道：“你不要打你，我\n",
      "[Epoch 34 Batch 50] loss 4.80, perplexity 121.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: "
     ]
    }
   ],
   "source": [
    "grad_clip = config.grad_clip\n",
    "seq_len = config.seq_len\n",
    "batch_size = config.batch_size\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(50):\n",
    "    total_loss = 0.0\n",
    "    hidden = model.begin_state(func=nd.zeros, batch_size=batch_size, ctx=context)\n",
    "    for ibatch, (data, label) in enumerate(train_data):\n",
    "        data = nd.array(data).as_in_context(context)\n",
    "        label = nd.array(label).as_in_context(context)\n",
    "        hidden = detach(hidden)\n",
    "        \n",
    "        with autograd.record(train_mode=True):\n",
    "            output, hidden = model(data, hidden)\n",
    "            loss = loss_func(output, label)\n",
    "            \n",
    "        loss.backward()\n",
    "        \n",
    "        grads = [x.grad(context) for x in model.collect_params().values()]\n",
    "        gluon.utils.clip_global_norm(grads, grad_clip * seq_len * batch_size)\n",
    "        \n",
    "        trainer.step(config.batch_size)\n",
    "        total_loss += nd.sum(loss).asscalar()\n",
    "        \n",
    "        if ibatch % config.print_per_batch == 0 and ibatch > 0:\n",
    "            cur_loss = total_loss / seq_len / batch_size / config.print_per_batch\n",
    "            print('[Epoch %d Batch %d] loss %.2f, perplexity %.2f' % (\n",
    "                    epoch + 1, ibatch, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0.0\n",
    "    print(''.join([corpus.dictionary.idx2word[x] for x in generate()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
